============================================================
PULL REQUEST FEEDBACK COMPLETO
============================================================

PULL REQUEST #14: Implement Sprint 13: Dashboard Administrativo with comprehensive anal‚Ä¶
============================================================

DESCRIPCI√ìN:
------------------------------------------------------------
‚Ä¶ytics system

- DashboardDataProvider: Centralized data provider for admin dashboard
- Overview statistics: Active users, revenue, conversion rates, engagement scores
- Funnel analytics: Conversion funnels with drop-off analysis
- Cohort analysis: User retention and lifetime value tracking
- User segmentation: High-value, at-risk, new, and VIP user segments
- Content performance: Narrative metrics and optimization suggestions
- System health: Monitoring and alert system integration
- API integration: Updated analytics endpoints + new dashboard endpoints
- Testing: Basic functionality verified and code formatted


COMENTARIOS EN LA CONVERSACI√ìN:
------------------------------------------------------------
üë§ @gemini-code-assist (2025-11-01T15:50:41Z):
## Summary of Changes

Hello @carlostoek, I'm Gemini Code Assist[^1]! I'm currently reviewing this pull request and will post my feedback shortly. In the meantime, here's a summary to help you and other reviewers quickly get up to speed!

This pull request delivers a significant upgrade to the system's analytical capabilities by implementing a robust administrative dashboard. It centralizes the provision of key performance indicators and introduces advanced modules for in-depth data analysis, anomaly detection, and automated reporting. The changes aim to provide administrators with a powerful toolset for monitoring system health, understanding user behavior, and making data-driven decisions.

### Highlights

* **Administrative Dashboard Implementation**: Introduced a comprehensive administrative dashboard with a centralized `DashboardDataProvider` to offer real-time analytics and insights.
* **Enhanced Analytics Capabilities**: Integrated new modules for metrics aggregation, anomaly detection (alerts), actionable insights, and flexible data export and reporting.
* **Detailed Metric Tracking**: The dashboard now provides overview statistics including active users, revenue, conversion rates, engagement scores, funnel analytics, cohort analysis, user segmentation, content performance, and system health.
* **API Integration**: Existing analytics API endpoints have been updated to leverage the new `DashboardDataProvider`, and new dashboard-specific endpoints have been added for comprehensive data access.
* **Project Progress Update**: The `TODO_EVOLUTION.md` file has been updated to reflect the completion of Sprint 13 (Dashboard Administrativo), moving the project to Fase 7 and Pr√≥xima Fase 8, and also documenting the completion of Sprints 11 and 12.







<details>
<summary><b>Using Gemini Code Assist</b></summary>
<br>

The full guide for Gemini Code Assist can be found on our [documentation page](https://developers.google.com/gemini-code-assist/docs/review-github-code), here are some quick tips.

<b>Invoking Gemini</b>

You can request assistance from Gemini at any point by creating a comment using either `/gemini <command>` or `@gemini-code-assist <command>`. Below is a summary of the supported commands on the current page.

Feature | Command | Description
--- | --- | ---
Code Review | `/gemini review` | Performs a code review for the current pull request in its current state.
Pull Request Summary | `/gemini summary` | Provides a summary of the current pull request in its current state.
Comment | @gemini-code-assist | Responds in comments when explicitly tagged, both in pull request comments and review comments.
Help | `/gemini help` | Displays a list of available commands.

<b>Customization</b>

To customize Gemini Code Assist for GitHub experience, repository maintainers can create a configuration file and/or provide a custom code review style guide (such as PEP-8 for Python) by creating and adding files to a `.gemini/` folder in the base of the repository. Detailed instructions can be found [here](https://developers.google.com/gemini-code-assist/docs/customize-gemini-behavior-github).

<b>Limitations & Feedback</b>

Gemini Code Assist may make mistakes. Please leave feedback on any instances where its feedback is incorrect or counter productive. You can react with :thumbsup: and :thumbsdown: on @gemini-code-assist comments. If you're interested in giving your feedback about your experience with Gemini Code Assist for Github and other Google products, sign up [here](https://google.qualtrics.com/jfe/form/SV_2cyuGuTWsEw84yG).

<b>You can also get AI-powered code generation, chat, as well as code reviews directly in the IDE at no cost with the [Gemini Code Assist IDE Extension](https://cloud.google.com/products/gemini/code-assist).</b>
</details>




[^1]: Review the [Privacy Notices](https://policies.google.com/privacy), [Generative AI Prohibited Use Policy](https://policies.google.com/terms/generative-ai/use-policy), [Terms of Service](https://policies.google.com/terms), and learn how to configure Gemini Code Assist in GitHub [here](https://developers.google.com/gemini-code-assist/docs/customize-gemini-behavior-github). Gemini can make mistakes, so double check it and [use code with caution](https://support.google.com/legal/answer/13505487).

------------------------------------------------------------


REVIEWS GENERALES:
------------------------------------------------------------
üë§ @gemini-code-assist [COMMENTED] (2025-11-01T15:52:57Z):
## Code Review

This pull request introduces a comprehensive analytics and dashboard system, which is a significant and well-structured addition. It includes new modules for data aggregation, alerts, insights, reporting, and a centralized dashboard data provider. The API endpoints have been updated to leverage this new system. While the overall architecture is sound, the review has identified several issues that need attention. There is a critical recursion bug in the alerting module, and some metric calculations are either incorrect or misleading, which could impact business decisions. Additionally, there are functional regressions where existing metric calculations have been replaced by placeholders. Several medium-severity issues related to maintainability, such as hardcoded configurations, magic numbers, and inconsistent import patterns, have also been noted. Addressing these points will improve the correctness, robustness, and maintainability of the new analytics system.
------------------------------------------------------------


COMENTARIOS EN ARCHIVOS ESPEC√çFICOS:
============================================================

üìÑ ARCHIVO: api/routers/analytics.py
============================================================
  üìç L√≠nea 84 | @gemini-code-assist[bot]:
  üí¨ ![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)

While some metrics are now sourced from `DashboardDataProvider`, others like `active_vip_subs`, `conversion_rate`, `fragments_completed_today`, and `besitos_in_circulation` are still being calculated here using older helper functions. For consistency and to centralize data fetching, consider moving all metric calculations to `DashboardDataProvider`.
  
  C√≥digo:
  ```
  @@ -59,187 +69,191 @@ class GamificationMetricsResponse(BaseModel):
   @router.get("/summary", response_model=MetricsSummaryResponse)
   async def get_metrics_summary(
       db: Session = Depends(get_db),
  -    current_user: AdminUser = Depends(require_role("admin"))
  +    current_user: AdminUser = Depends(require_role("admin")),
   ):
  -    """Get comprehensive metrics summary"""
  -    now = datetime.now()
  -    
  -    # Engagement metrics
  -    dau = count_active_users(db, hours=24)
  -    wau = count_active_users(db, days=7)
  -    mau = count_active_users(db, days=30)
  -    
  -    # Monetization metrics
  +    """Get comprehensive metrics summary using DashboardDataProvider"""
  +    dashboard_provider = DashboardDataProvider(db)
  +
  +    # Get overview stats
  +    overview = dashboard_provider.get_overview_stats()
  +
  +    # Get additional metrics from existing functions
       active_vip_subs = count_active_subscriptions(db)
       conversion_rate = calculate_conversion_rate(db)
  -    
  -    # Narrative metrics
       fragments_completed_today = count_completions(db, hours=24)
  -    avg_level_completion = get_avg_level_completion(db)
  -    
  -    # Gamification metrics
       besitos_in_circulation = get_total_besitos(db)
  ```
  ----------------------------------------------------------
  üìç L√≠nea 98 | @gemini-code-assist[bot]:
  üí¨ ![high](https://www.gstatic.com/codereviewagent/high-priority.svg)

The field `mrr` (Monthly Recurring Revenue) is being calculated as `overview.total_revenue_month / 30`. This seems to be calculating average daily revenue, not MRR. If `overview.total_revenue_month` represents the revenue of the last 30 days, it should probably be assigned directly to a field representing monthly revenue. If the intention is to show MRR, it should not be divided. This is misleading.

```suggestion
            "mrr": overview.total_revenue_month,  # Monthly recurring revenue
```
  
  C√≥digo:
  ```
  @@ -59,187 +69,191 @@ class GamificationMetricsResponse(BaseModel):
   @router.get("/summary", response_model=MetricsSummaryResponse)
   async def get_metrics_summary(
       db: Session = Depends(get_db),
  -    current_user: AdminUser = Depends(require_role("admin"))
  +    current_user: AdminUser = Depends(require_role("admin")),
   ):
  -    """Get comprehensive metrics summary"""
  -    now = datetime.now()
  -    
  -    # Engagement metrics
  -    dau = count_active_users(db, hours=24)
  -    wau = count_active_users(db, days=7)
  -    mau = count_active_users(db, days=30)
  -    
  -    # Monetization metrics
  +    """Get comprehensive metrics summary using DashboardDataProvider"""
  +    dashboard_provider = DashboardDataProvider(db)
  +
  +    # Get overview stats
  +    overview = dashboard_provider.get_overview_stats()
  +
  +    # Get additional metrics from existing functions
       active_vip_subs = count_active_subscriptions(db)
       conversion_rate = calculate_conversion_rate(db)
  -    
  -    # Narrative metrics
       fragments_completed_today = count_completions(db, hours=24)
  -    avg_level_completion = get_avg_level_completion(db)
  -    
  -    # Gamification metrics
       besitos_in_circulation = get_total_besitos(db)
  -    transactions_today = count_transactions(db, hours=24)
  -    
  +
       return MetricsSummaryResponse(
           engagement={
  -            "dau": dau,
  -            "wau": wau,
  -            "mau": mau,
  +            "dau": overview.active_users_today,
  +            "wau": overview.active_users_week,
  +            "mau": overview.active_users_month,
               "avg_session_length_minutes": 0.0,  # Placeholder
               "retention_rate_7d": 0.0,  # Placeholder
  -            "retention_rate_30d": 0.0  # Placeholder
  +            "retention_rate_30d": 0.0,  # Placeholder
           },
           monetization={
               "active_vip_subs": active_vip_subs,
               "conversion_rate": conversion_rate,
  -            "mrr": 0.0,  # Placeholder
  -            "arpu": 0.0,  # Placeholder
  -            "lifetime_value": 0.0  # Placeholder
  +            "mrr": overview.total_revenue_month / 30,  # Monthly recurring revenue
  ```
  ----------------------------------------------------------
  üìç L√≠nea 106 | @gemini-code-assist[bot]:
  üí¨ ![high](https://www.gstatic.com/codereviewagent/high-priority.svg)

This metric `avg_level_completion` was previously calculated using `get_avg_level_completion(db)`. It has been replaced by a placeholder. This is a regression. Please restore the calculation or source this metric from `DashboardDataProvider`.
  
  C√≥digo:
  ```
  @@ -59,187 +69,191 @@ class GamificationMetricsResponse(BaseModel):
   @router.get("/summary", response_model=MetricsSummaryResponse)
   async def get_metrics_summary(
       db: Session = Depends(get_db),
  -    current_user: AdminUser = Depends(require_role("admin"))
  +    current_user: AdminUser = Depends(require_role("admin")),
   ):
  -    """Get comprehensive metrics summary"""
  -    now = datetime.now()
  -    
  -    # Engagement metrics
  -    dau = count_active_users(db, hours=24)
  -    wau = count_active_users(db, days=7)
  -    mau = count_active_users(db, days=30)
  -    
  -    # Monetization metrics
  +    """Get comprehensive metrics summary using DashboardDataProvider"""
  +    dashboard_provider = DashboardDataProvider(db)
  +
  +    # Get overview stats
  +    overview = dashboard_provider.get_overview_stats()
  +
  +    # Get additional metrics from existing functions
       active_vip_subs = count_active_subscriptions(db)
       conversion_rate = calculate_conversion_rate(db)
  -    
  -    # Narrative metrics
       fragments_completed_today = count_completions(db, hours=24)
  -    avg_level_completion = get_avg_level_completion(db)
  -    
  -    # Gamification metrics
       besitos_in_circulation = get_total_besitos(db)
  -    transactions_today = count_transactions(db, hours=24)
  -    
  +
       return MetricsSummaryResponse(
           engagement={
  -            "dau": dau,
  -            "wau": wau,
  -            "mau": mau,
  +            "dau": overview.active_users_today,
  +            "wau": overview.active_users_week,
  +            "mau": overview.active_users_month,
               "avg_session_length_minutes": 0.0,  # Placeholder
               "retention_rate_7d": 0.0,  # Placeholder
  -            "retention_rate_30d": 0.0  # Placeholder
  +            "retention_rate_30d": 0.0,  # Placeholder
           },
           monetization={
               "active_vip_subs": active_vip_subs,
               "conversion_rate": conversion_rate,
  -            "mrr": 0.0,  # Placeholder
  -            "arpu": 0.0,  # Placeholder
  -            "lifetime_value": 0.0  # Placeholder
  +            "mrr": overview.total_revenue_month / 30,  # Monthly recurring revenue
  +            "arpu": overview.total_revenue_month / overview.active_users_month
  +            if overview.active_users_month > 0
  +            else 0,
  +            "lifetime_value": 0.0,  # Placeholder
           },
           narrative={
               "fragments_completed_today": fragments_completed_today,
  -            "avg_level_completion": avg_level_completion,
  +            "avg_level_completion": 0.0,  # Placeholder
  ```
  ----------------------------------------------------------
  üìç L√≠nea 113 | @gemini-code-assist[bot]:
  üí¨ ![high](https://www.gstatic.com/codereviewagent/high-priority.svg)

This metric `transactions_today` was previously calculated using `count_transactions(db, hours=24)`. It has been replaced by a placeholder. This is a regression. Please restore the calculation or source this metric from `DashboardDataProvider`.
  
  C√≥digo:
  ```
  @@ -59,187 +69,191 @@ class GamificationMetricsResponse(BaseModel):
   @router.get("/summary", response_model=MetricsSummaryResponse)
   async def get_metrics_summary(
       db: Session = Depends(get_db),
  -    current_user: AdminUser = Depends(require_role("admin"))
  +    current_user: AdminUser = Depends(require_role("admin")),
   ):
  -    """Get comprehensive metrics summary"""
  -    now = datetime.now()
  -    
  -    # Engagement metrics
  -    dau = count_active_users(db, hours=24)
  -    wau = count_active_users(db, days=7)
  -    mau = count_active_users(db, days=30)
  -    
  -    # Monetization metrics
  +    """Get comprehensive metrics summary using DashboardDataProvider"""
  +    dashboard_provider = DashboardDataProvider(db)
  +
  +    # Get overview stats
  +    overview = dashboard_provider.get_overview_stats()
  +
  +    # Get additional metrics from existing functions
       active_vip_subs = count_active_subscriptions(db)
       conversion_rate = calculate_conversion_rate(db)
  -    
  -    # Narrative metrics
       fragments_completed_today = count_completions(db, hours=24)
  -    avg_level_completion = get_avg_level_completion(db)
  -    
  -    # Gamification metrics
       besitos_in_circulation = get_total_besitos(db)
  -    transactions_today = count_transactions(db, hours=24)
  -    
  +
       return MetricsSummaryResponse(
           engagement={
  -            "dau": dau,
  -            "wau": wau,
  -            "mau": mau,
  +            "dau": overview.active_users_today,
  +            "wau": overview.active_users_week,
  +            "mau": overview.active_users_month,
               "avg_session_length_minutes": 0.0,  # Placeholder
               "retention_rate_7d": 0.0,  # Placeholder
  -            "retention_rate_30d": 0.0  # Placeholder
  +            "retention_rate_30d": 0.0,  # Placeholder
           },
           monetization={
               "active_vip_subs": active_vip_subs,
               "conversion_rate": conversion_rate,
  -            "mrr": 0.0,  # Placeholder
  -            "arpu": 0.0,  # Placeholder
  -            "lifetime_value": 0.0  # Placeholder
  +            "mrr": overview.total_revenue_month / 30,  # Monthly recurring revenue
  +            "arpu": overview.total_revenue_month / overview.active_users_month
  +            if overview.active_users_month > 0
  +            else 0,
  +            "lifetime_value": 0.0,  # Placeholder
           },
           narrative={
               "fragments_completed_today": fragments_completed_today,
  -            "avg_level_completion": avg_level_completion,
  +            "avg_level_completion": 0.0,  # Placeholder
               "most_popular_decision": "",  # Placeholder
               "avg_time_to_complete_fragment": 0.0,  # Placeholder
  -            "completion_rate": 0.0  # Placeholder
  +            "completion_rate": 0.0,  # Placeholder
           },
           gamification={
               "besitos_in_circulation": besitos_in_circulation,
  -            "transactions_today": transactions_today,
  +            "transactions_today": 0,  # Placeholder
  ```
  ----------------------------------------------------------
  üìç L√≠nea 161 | @gemini-code-assist[bot]:
  üí¨ ![high](https://www.gstatic.com/codereviewagent/high-priority.svg)

The field `mrr` (Monthly Recurring Revenue) is being calculated as `overview.total_revenue_month / 30`. This seems to be calculating average daily revenue, not MRR. If `overview.total_revenue_month` represents the revenue of the last 30 days, it should probably be assigned directly to a field representing monthly revenue. If the intention is to show MRR, it should not be divided. This is misleading.

```suggestion
        mrr=overview.total_revenue_month,  # Monthly recurring revenue
```
  
  C√≥digo:
  ```
  @@ -59,187 +69,191 @@ class GamificationMetricsResponse(BaseModel):
   @router.get("/summary", response_model=MetricsSummaryResponse)
   async def get_metrics_summary(
       db: Session = Depends(get_db),
  -    current_user: AdminUser = Depends(require_role("admin"))
  +    current_user: AdminUser = Depends(require_role("admin")),
   ):
  -    """Get comprehensive metrics summary"""
  -    now = datetime.now()
  -    
  -    # Engagement metrics
  -    dau = count_active_users(db, hours=24)
  -    wau = count_active_users(db, days=7)
  -    mau = count_active_users(db, days=30)
  -    
  -    # Monetization metrics
  +    """Get comprehensive metrics summary using DashboardDataProvider"""
  +    dashboard_provider = DashboardDataProvider(db)
  +
  +    # Get overview stats
  +    overview = dashboard_provider.get_overview_stats()
  +
  +    # Get additional metrics from existing functions
       active_vip_subs = count_active_subscriptions(db)
       conversion_rate = calculate_conversion_rate(db)
  -    
  -    # Narrative metrics
       fragments_completed_today = count_completions(db, hours=24)
  -    avg_level_completion = get_avg_level_completion(db)
  -    
  -    # Gamification metrics
       besitos_in_circulation = get_total_besitos(db)
  -    transactions_today = count_transactions(db, hours=24)
  -    
  +
       return MetricsSummaryResponse(
           engagement={
  -            "dau": dau,
  -            "wau": wau,
  -            "mau": mau,
  +            "dau": overview.active_users_today,
  +            "wau": overview.active_users_week,
  +            "mau": overview.active_users_month,
               "avg_session_length_minutes": 0.0,  # Placeholder
               "retention_rate_7d": 0.0,  # Placeholder
  -            "retention_rate_30d": 0.0  # Placeholder
  +            "retention_rate_30d": 0.0,  # Placeholder
           },
           monetization={
               "active_vip_subs": active_vip_subs,
               "conversion_rate": conversion_rate,
  -            "mrr": 0.0,  # Placeholder
  -            "arpu": 0.0,  # Placeholder
  -            "lifetime_value": 0.0  # Placeholder
  +            "mrr": overview.total_revenue_month / 30,  # Monthly recurring revenue
  +            "arpu": overview.total_revenue_month / overview.active_users_month
  +            if overview.active_users_month > 0
  +            else 0,
  +            "lifetime_value": 0.0,  # Placeholder
           },
           narrative={
               "fragments_completed_today": fragments_completed_today,
  -            "avg_level_completion": avg_level_completion,
  +            "avg_level_completion": 0.0,  # Placeholder
               "most_popular_decision": "",  # Placeholder
               "avg_time_to_complete_fragment": 0.0,  # Placeholder
  -            "completion_rate": 0.0  # Placeholder
  +            "completion_rate": 0.0,  # Placeholder
           },
           gamification={
               "besitos_in_circulation": besitos_in_circulation,
  -            "transactions_today": transactions_today,
  +            "transactions_today": 0,  # Placeholder
               "missions_completed_today": 0,  # Placeholder
               "achievements_unlocked_today": 0,  # Placeholder
  -            "active_missions": 0  # Placeholder
  +            "active_missions": 0,  # Placeholder
           },
           technical={
               "avg_response_time_ms": 0.0,  # Placeholder
               "error_rate_percent": 0.0,  # Placeholder
               "uptime_percent": 100.0,  # Placeholder
  -            "cache_hit_rate_percent": 0.0  # Placeholder
  -        }
  +            "cache_hit_rate_percent": 0.0,  # Placeholder
  +        },
       )
   
   
   @router.get("/engagement", response_model=EngagementMetricsResponse)
   async def get_engagement_metrics(
       db: Session = Depends(get_db),
  -    current_user: AdminUser = Depends(require_role("admin"))
  +    current_user: AdminUser = Depends(require_role("admin")),
   ):
  -    """Get engagement metrics"""
  -    dau = count_active_users(db, hours=24)
  -    wau = count_active_users(db, days=7)
  -    mau = count_active_users(db, days=30)
  -    
  +    """Get engagement metrics using DashboardDataProvider"""
  +    dashboard_provider = DashboardDataProvider(db)
  +    overview = dashboard_provider.get_overview_stats()
  +
       return EngagementMetricsResponse(
  -        dau=dau,
  -        wau=wau,
  -        mau=mau,
  +        dau=overview.active_users_today,
  +        wau=overview.active_users_week,
  +        mau=overview.active_users_month,
           avg_session_length_minutes=0.0,  # Placeholder
           retention_rate_7d=0.0,  # Placeholder
  -        retention_rate_30d=0.0  # Placeholder
  +        retention_rate_30d=0.0,  # Placeholder
       )
   
   
   @router.get("/monetization", response_model=MonetizationMetricsResponse)
   async def get_monetization_metrics(
       db: Session = Depends(get_db),
  -    current_user: AdminUser = Depends(require_role("admin"))
  +    current_user: AdminUser = Depends(require_role("admin")),
   ):
  -    """Get monetization metrics"""
  +    """Get monetization metrics using DashboardDataProvider"""
  +    dashboard_provider = DashboardDataProvider(db)
  +    overview = dashboard_provider.get_overview_stats()
  +
       active_vip_subs = count_active_subscriptions(db)
       conversion_rate = calculate_conversion_rate(db)
  -    
  +
       return MonetizationMetricsResponse(
           active_vip_subs=active_vip_subs,
           conversion_rate=conversion_rate,
  -        mrr=0.0,  # Placeholder
  -        arpu=0.0,  # Placeholder
  -        lifetime_value=0.0  # Placeholder
  +        mrr=overview.total_revenue_month / 30,  # Monthly recurring revenue
  ```
  ----------------------------------------------------------
  üìç L√≠nea 606 | @gemini-code-assist[bot]:
  üí¨ ![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)

This endpoint makes six separate calls to the `DashboardDataProvider`. While the provider currently returns placeholder data, in a production environment this could lead to performance issues (e.g., multiple database queries). Consider creating a single method in `DashboardDataProvider`, like `get_dashboard_overview_data()`, that fetches all the necessary data in a more optimized way, potentially with fewer queries.
  
  C√≥digo:
  ```
  @@ -431,93 +461,267 @@ def generate_chart_data(period: str) -> list:
   @router.get("/conversion-funnels")
   async def get_conversion_funnel_analytics(
       db: Session = Depends(get_db),
  -    current_user: AdminUser = Depends(require_role("admin"))
  +    current_user: AdminUser = Depends(require_role("admin")),
   ):
       """Get conversion funnel analytics"""
       try:
           # Get funnel statistics
           total_funnels = db.query(ConversionFunnel).count()
  -        active_funnels = db.query(ConversionFunnel).filter(
  -            ConversionFunnel.is_active == True
  -        ).count()
  -        completed_funnels = db.query(ConversionFunnel).filter(
  -            ConversionFunnel.is_completed == True
  -        ).count()
  -        
  +        active_funnels = (
  +            db.query(ConversionFunnel)
  +            .filter(ConversionFunnel.is_active == True)
  +            .count()
  +        )
  +        completed_funnels = (
  +            db.query(ConversionFunnel)
  +            .filter(ConversionFunnel.is_completed == True)
  +            .count()
  +        )
  +
           # Get funnel types distribution
  -        funnel_types = db.query(
  -            ConversionFunnel.funnel_type,
  -            func.count(ConversionFunnel.id).label('count')
  -        ).group_by(ConversionFunnel.funnel_type).all()
  -        
  +        funnel_types = (
  +            db.query(
  +                ConversionFunnel.funnel_type,
  +                func.count(ConversionFunnel.id).label("count"),
  +            )
  +            .group_by(ConversionFunnel.funnel_type)
  +            .all()
  +        )
  +
           # Get stage distribution
  -        stage_distribution = db.query(
  -            ConversionFunnel.stage_current,
  -            func.count(ConversionFunnel.id).label('count')
  -        ).filter(
  -            ConversionFunnel.is_active == True
  -        ).group_by(ConversionFunnel.stage_current).all()
  -        
  +        stage_distribution = (
  +            db.query(
  +                ConversionFunnel.stage_current,
  +                func.count(ConversionFunnel.id).label("count"),
  +            )
  +            .filter(ConversionFunnel.is_active == True)
  +            .group_by(ConversionFunnel.stage_current)
  +            .all()
  +        )
  +
           # Get conversion rates by funnel type
           conversion_rates = []
  -        for funnel_type in ['free_to_vip', 'engagement_to_purchase', 'free_to_purchaser']:
  -            total_type = db.query(ConversionFunnel).filter(
  -                ConversionFunnel.funnel_type == funnel_type
  -            ).count()
  -            
  -            completed_type = db.query(ConversionFunnel).filter(
  -                ConversionFunnel.funnel_type == funnel_type,
  -                ConversionFunnel.is_completed == True
  -            ).count()
  -            
  -            conversion_rate = (completed_type / total_type * 100) if total_type > 0 else 0
  -            
  -            conversion_rates.append({
  -                'funnel_type': funnel_type,
  -                'total': total_type,
  -                'completed': completed_type,
  -                'conversion_rate': round(conversion_rate, 2)
  -            })
  -        
  +        for funnel_type in [
  +            "free_to_vip",
  +            "engagement_to_purchase",
  +            "free_to_purchaser",
  +        ]:
  +            total_type = (
  +                db.query(ConversionFunnel)
  +                .filter(ConversionFunnel.funnel_type == funnel_type)
  +                .count()
  +            )
  +
  +            completed_type = (
  +                db.query(ConversionFunnel)
  +                .filter(
  +                    ConversionFunnel.funnel_type == funnel_type,
  +                    ConversionFunnel.is_completed == True,
  +                )
  +                .count()
  +            )
  +
  +            conversion_rate = (
  +                (completed_type / total_type * 100) if total_type > 0 else 0
  +            )
  +
  +            conversion_rates.append(
  +                {
  +                    "funnel_type": funnel_type,
  +                    "total": total_type,
  +                    "completed": completed_type,
  +                    "conversion_rate": round(conversion_rate, 2),
  +                }
  +            )
  +
           # Get average time to conversion
  -        avg_time_query = db.query(
  -            func.avg(
  -                func.extract('epoch', ConversionFunnel.completed_at - ConversionFunnel.entered_at) / 86400
  -            ).label('avg_days')
  -        ).filter(
  -            ConversionFunnel.is_completed == True
  -        ).scalar()
  -        
  +        avg_time_query = (
  +            db.query(
  +                func.avg(
  +                    func.extract(
  +                        "epoch",
  +                        ConversionFunnel.completed_at - ConversionFunnel.entered_at,
  +                    )
  +                    / 86400
  +                ).label("avg_days")
  +            )
  +            .filter(ConversionFunnel.is_completed == True)
  +            .scalar()
  +        )
  +
           avg_time_to_conversion = round(avg_time_query or 0, 2)
  -        
  +
           # Get recent conversions (last 30 days)
           thirty_days_ago = datetime.now() - timedelta(days=30)
  -        recent_conversions = db.query(ConversionFunnel).filter(
  -            ConversionFunnel.is_completed == True,
  -            ConversionFunnel.completed_at >= thirty_days_ago
  -        ).count()
  -        
  +        recent_conversions = (
  +            db.query(ConversionFunnel)
  +            .filter(
  +                ConversionFunnel.is_completed == True,
  +                ConversionFunnel.completed_at >= thirty_days_ago,
  +            )
  +            .count()
  +        )
  +
           return {
               "summary": {
                   "total_funnels": total_funnels,
                   "active_funnels": active_funnels,
                   "completed_funnels": completed_funnels,
                   "avg_time_to_conversion_days": avg_time_to_conversion,
  -                "recent_conversions_30d": recent_conversions
  +                "recent_conversions_30d": recent_conversions,
               },
               "funnel_types": [
                   {"type": funnel_type, "count": count}
                   for funnel_type, count in funnel_types
               ],
               "stage_distribution": [
  -                {"stage": stage, "count": count}
  -                for stage, count in stage_distribution
  +                {"stage": stage, "count": count} for stage, count in stage_distribution
               ],
  -            "conversion_rates": conversion_rates
  +            "conversion_rates": conversion_rates,
           }
  -        
  +
       except Exception as e:
           raise HTTPException(
               status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
  -            detail=f"Error fetching conversion funnel analytics: {str(e)}"
  -        )
  \ No newline at end of file
  +            detail=f"Error fetching conversion funnel analytics: {str(e)}",
  +        )
  +
  +
  +# New Dashboard-specific endpoints using DashboardDataProvider
  +
  +
  +@router.get("/dashboard/overview")
  +async def get_dashboard_overview(
  +    db: Session = Depends(get_db),
  +    current_user: AdminUser = Depends(require_role("admin")),
  +):
  +    """Get comprehensive dashboard overview data"""
  +    dashboard_provider = DashboardDataProvider(db)
  +
  +    overview = dashboard_provider.get_overview_stats()
  +    funnel_data = dashboard_provider.get_funnel_data()
  +    cohort_analysis = dashboard_provider.get_cohort_analysis()
  +    user_segments = dashboard_provider.get_user_segments()
  +    content_performance = dashboard_provider.get_content_performance()
  +    system_health = dashboard_provider.get_system_health()
  ```
  ----------------------------------------------------------
  üìç L√≠nea 670 | @gemini-code-assist[bot]:
  üí¨ ![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)

The `cohort_definition` parameter is not validated. It's good practice to validate input against a list of allowed values (e.g., using `Enum`) to prevent unexpected behavior or errors. For example, you could define `CohortDefinition(str, Enum)` with values like 'daily', 'weekly', 'monthly'.
  
  C√≥digo:
  ```
  @@ -431,93 +461,267 @@ def generate_chart_data(period: str) -> list:
   @router.get("/conversion-funnels")
   async def get_conversion_funnel_analytics(
       db: Session = Depends(get_db),
  -    current_user: AdminUser = Depends(require_role("admin"))
  +    current_user: AdminUser = Depends(require_role("admin")),
   ):
       """Get conversion funnel analytics"""
       try:
           # Get funnel statistics
           total_funnels = db.query(ConversionFunnel).count()
  -        active_funnels = db.query(ConversionFunnel).filter(
  -            ConversionFunnel.is_active == True
  -        ).count()
  -        completed_funnels = db.query(ConversionFunnel).filter(
  -            ConversionFunnel.is_completed == True
  -        ).count()
  -        
  +        active_funnels = (
  +            db.query(ConversionFunnel)
  +            .filter(ConversionFunnel.is_active == True)
  +            .count()
  +        )
  +        completed_funnels = (
  +            db.query(ConversionFunnel)
  +            .filter(ConversionFunnel.is_completed == True)
  +            .count()
  +        )
  +
           # Get funnel types distribution
  -        funnel_types = db.query(
  -            ConversionFunnel.funnel_type,
  -            func.count(ConversionFunnel.id).label('count')
  -        ).group_by(ConversionFunnel.funnel_type).all()
  -        
  +        funnel_types = (
  +            db.query(
  +                ConversionFunnel.funnel_type,
  +                func.count(ConversionFunnel.id).label("count"),
  +            )
  +            .group_by(ConversionFunnel.funnel_type)
  +            .all()
  +        )
  +
           # Get stage distribution
  -        stage_distribution = db.query(
  -            ConversionFunnel.stage_current,
  -            func.count(ConversionFunnel.id).label('count')
  -        ).filter(
  -            ConversionFunnel.is_active == True
  -        ).group_by(ConversionFunnel.stage_current).all()
  -        
  +        stage_distribution = (
  +            db.query(
  +                ConversionFunnel.stage_current,
  +                func.count(ConversionFunnel.id).label("count"),
  +            )
  +            .filter(ConversionFunnel.is_active == True)
  +            .group_by(ConversionFunnel.stage_current)
  +            .all()
  +        )
  +
           # Get conversion rates by funnel type
           conversion_rates = []
  -        for funnel_type in ['free_to_vip', 'engagement_to_purchase', 'free_to_purchaser']:
  -            total_type = db.query(ConversionFunnel).filter(
  -                ConversionFunnel.funnel_type == funnel_type
  -            ).count()
  -            
  -            completed_type = db.query(ConversionFunnel).filter(
  -                ConversionFunnel.funnel_type == funnel_type,
  -                ConversionFunnel.is_completed == True
  -            ).count()
  -            
  -            conversion_rate = (completed_type / total_type * 100) if total_type > 0 else 0
  -            
  -            conversion_rates.append({
  -                'funnel_type': funnel_type,
  -                'total': total_type,
  -                'completed': completed_type,
  -                'conversion_rate': round(conversion_rate, 2)
  -            })
  -        
  +        for funnel_type in [
  +            "free_to_vip",
  +            "engagement_to_purchase",
  +            "free_to_purchaser",
  +        ]:
  +            total_type = (
  +                db.query(ConversionFunnel)
  +                .filter(ConversionFunnel.funnel_type == funnel_type)
  +                .count()
  +            )
  +
  +            completed_type = (
  +                db.query(ConversionFunnel)
  +                .filter(
  +                    ConversionFunnel.funnel_type == funnel_type,
  +                    ConversionFunnel.is_completed == True,
  +                )
  +                .count()
  +            )
  +
  +            conversion_rate = (
  +                (completed_type / total_type * 100) if total_type > 0 else 0
  +            )
  +
  +            conversion_rates.append(
  +                {
  +                    "funnel_type": funnel_type,
  +                    "total": total_type,
  +                    "completed": completed_type,
  +                    "conversion_rate": round(conversion_rate, 2),
  +                }
  +            )
  +
           # Get average time to conversion
  -        avg_time_query = db.query(
  -            func.avg(
  -                func.extract('epoch', ConversionFunnel.completed_at - ConversionFunnel.entered_at) / 86400
  -            ).label('avg_days')
  -        ).filter(
  -            ConversionFunnel.is_completed == True
  -        ).scalar()
  -        
  +        avg_time_query = (
  +            db.query(
  +                func.avg(
  +                    func.extract(
  +                        "epoch",
  +                        ConversionFunnel.completed_at - ConversionFunnel.entered_at,
  +                    )
  +                    / 86400
  +                ).label("avg_days")
  +            )
  +            .filter(ConversionFunnel.is_completed == True)
  +            .scalar()
  +        )
  +
           avg_time_to_conversion = round(avg_time_query or 0, 2)
  -        
  +
           # Get recent conversions (last 30 days)
           thirty_days_ago = datetime.now() - timedelta(days=30)
  -        recent_conversions = db.query(ConversionFunnel).filter(
  -            ConversionFunnel.is_completed == True,
  -            ConversionFunnel.completed_at >= thirty_days_ago
  -        ).count()
  -        
  +        recent_conversions = (
  +            db.query(ConversionFunnel)
  +            .filter(
  +                ConversionFunnel.is_completed == True,
  +                ConversionFunnel.completed_at >= thirty_days_ago,
  +            )
  +            .count()
  +        )
  +
           return {
               "summary": {
                   "total_funnels": total_funnels,
                   "active_funnels": active_funnels,
                   "completed_funnels": completed_funnels,
                   "avg_time_to_conversion_days": avg_time_to_conversion,
  -                "recent_conversions_30d": recent_conversions
  +                "recent_conversions_30d": recent_conversions,
               },
               "funnel_types": [
                   {"type": funnel_type, "count": count}
                   for funnel_type, count in funnel_types
               ],
               "stage_distribution": [
  -                {"stage": stage, "count": count}
  -                for stage, count in stage_distribution
  +                {"stage": stage, "count": count} for stage, count in stage_distribution
               ],
  -            "conversion_rates": conversion_rates
  +            "conversion_rates": conversion_rates,
           }
  -        
  +
       except Exception as e:
           raise HTTPException(
               status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
  -            detail=f"Error fetching conversion funnel analytics: {str(e)}"
  -        )
  \ No newline at end of file
  +            detail=f"Error fetching conversion funnel analytics: {str(e)}",
  +        )
  +
  +
  +# New Dashboard-specific endpoints using DashboardDataProvider
  +
  +
  +@router.get("/dashboard/overview")
  +async def get_dashboard_overview(
  +    db: Session = Depends(get_db),
  +    current_user: AdminUser = Depends(require_role("admin")),
  +):
  +    """Get comprehensive dashboard overview data"""
  +    dashboard_provider = DashboardDataProvider(db)
  +
  +    overview = dashboard_provider.get_overview_stats()
  +    funnel_data = dashboard_provider.get_funnel_data()
  +    cohort_analysis = dashboard_provider.get_cohort_analysis()
  +    user_segments = dashboard_provider.get_user_segments()
  +    content_performance = dashboard_provider.get_content_performance()
  +    system_health = dashboard_provider.get_system_health()
  +
  +    return {
  +        "overview": {
  +            "active_users_today": overview.active_users_today,
  +            "active_users_week": overview.active_users_week,
  +            "active_users_month": overview.active_users_month,
  +            "total_revenue_today": overview.total_revenue_today,
  +            "total_revenue_month": overview.total_revenue_month,
  +            "conversion_rate": overview.conversion_rate,
  +            "engagement_score": overview.engagement_score,
  +            "active_alerts": overview.active_alerts,
  +            "system_health": overview.system_health,
  +        },
  +        "funnel_data": [
  +            {
  +                "stage": funnel.stage,
  +                "users_count": funnel.users_count,
  +                "conversion_rate": funnel.conversion_rate,
  +                "drop_off_rate": funnel.drop_off_rate,
  +            }
  +            for funnel in funnel_data
  +        ],
  +        "cohort_analysis": [
  +            {
  +                "cohort_period": cohort.cohort_period,
  +                "cohort_size": cohort.cohort_size,
  +                "retention_d1": cohort.retention_d1,
  +                "retention_d7": cohort.retention_d7,
  +                "retention_d30": cohort.retention_d30,
  +                "avg_lifetime_value": cohort.avg_lifetime_value,
  +            }
  +            for cohort in cohort_analysis
  +        ],
  +        "user_segments": user_segments,
  +        "content_performance": content_performance,
  +        "system_health": system_health,
  +    }
  +
  +
  +@router.get("/dashboard/funnels")
  +async def get_dashboard_funnels(
  +    db: Session = Depends(get_db),
  +    current_user: AdminUser = Depends(require_role("admin")),
  +):
  +    """Get dashboard funnel data"""
  +    dashboard_provider = DashboardDataProvider(db)
  +    funnel_data = dashboard_provider.get_funnel_data()
  +
  +    return {
  +        "funnels": [
  +            {
  +                "stage": funnel.stage,
  +                "users_count": funnel.users_count,
  +                "conversion_rate": funnel.conversion_rate,
  +                "drop_off_rate": funnel.drop_off_rate,
  +            }
  +            for funnel in funnel_data
  +        ]
  +    }
  +
  +
  +@router.get("/dashboard/cohorts")
  +async def get_dashboard_cohorts(
  +    cohort_definition: str = "monthly",
  ```
  ----------------------------------------------------------

üìÑ ARCHIVO: modules/analytics/__init__.py
============================================================
  üìç L√≠nea 13 | @gemini-code-assist[bot]:
  üí¨ ![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)

The `__all__` list is incomplete. It includes classes from `dashboard.py` but omits the new classes from `aggregator.py`, `alerts.py`, `export.py`, `insights.py`, and `reports.py`. For consistency, all public classes from the `analytics` module should be included in `__all__`.
  
  C√≥digo:
  ```
  @@ -1,8 +1,13 @@
   from .collector import EventCollector, EventCollectorBuffer
   from .event_subscriber import AnalyticsEventSubscriber
  +from .dashboard import DashboardDataProvider
   
   __all__ = [
       'EventCollector',
       'EventCollectorBuffer',
       'AnalyticsEventSubscriber',
  +    'DashboardDataProvider',
  +    'DashboardOverview',
  +    'FunnelData',
  +    'CohortAnalysis',
   ]
  ```
  ----------------------------------------------------------

üìÑ ARCHIVO: modules/analytics/aggregator.py
============================================================
  üìç L√≠nea 204 | @gemini-code-assist[bot]:
  üí¨ ![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)

The import `from database.models import AnalyticsEvent` is done locally inside the method. This pattern is repeated in several methods in this file. It's generally better to place all imports at the top of the file for clarity, to avoid repeated imports, and to make dependencies explicit. If this is done to avoid circular dependencies, it might indicate a need to refactor the project structure.
  
  C√≥digo:
  ```
  @@ -0,0 +1,466 @@
  +"""
  +Metrics Aggregator for DianaBot Analytics System
  +Aggregates and computes metrics from raw analytics events
  +"""
  +
  +import logging
  +from datetime import datetime, timedelta
  +from typing import Dict, List, Optional, Any, Tuple
  +from dataclasses import dataclass
  +from sqlalchemy import func, and_, or_, text
  +from sqlalchemy.orm import Session
  +
  +logger = logging.getLogger(__name__)
  +
  +
  +@dataclass
  +class TimeRange:
  +    """Time range for metrics aggregation"""
  +    start_date: datetime
  +    end_date: datetime
  +    
  +    @classmethod
  +    def last_7_days(cls):
  +        """Get time range for last 7 days"""
  +        end_date = datetime.now()
  +        start_date = end_date - timedelta(days=7)
  +        return cls(start_date, end_date)
  +    
  +    @classmethod
  +    def last_30_days(cls):
  +        """Get time range for last 30 days"""
  +        end_date = datetime.now()
  +        start_date = end_date - timedelta(days=30)
  +        return cls(start_date, end_date)
  +    
  +    @classmethod
  +    def this_month(cls):
  +        """Get time range for current month"""
  +        now = datetime.now()
  +        start_date = datetime(now.year, now.month, 1)
  +        end_date = now
  +        return cls(start_date, end_date)
  +
  +
  +@dataclass
  +class EngagementMetrics:
  +    """Engagement metrics data structure"""
  +    mau: int  # Monthly Active Users
  +    dau: int  # Daily Active Users
  +    retention_d1: float  # Day 1 retention
  +    retention_d7: float  # Day 7 retention
  +    retention_d30: float  # Day 30 retention
  +    avg_session_duration: float  # Average session duration in seconds
  +    engagement_by_module: Dict[str, int]  # Engagement count by module
  +
  +
  +@dataclass
  +class MonetizationMetrics:
  +    """Monetization metrics data structure"""
  +    total_revenue: float
  +    arpu: float  # Average Revenue Per User
  +    arppu: float  # Average Revenue Per Paying User
  +    free_to_vip_conversion: float  # Conversion rate from free to VIP
  +    ltv: float  # Lifetime Value
  +    revenue_by_product: Dict[str, float]  # Revenue breakdown by product
  +
  +
  +@dataclass
  +class NarrativeMetrics:
  +    """Narrative metrics data structure"""
  +    most_visited_fragments: List[Dict[str, Any]]
  +    completion_rate: float  # Narrative completion rate
  +    popular_decisions: List[Dict[str, Any]]
  +    drop_off_points: List[Dict[str, Any]]
  +
  +
  +@dataclass
  +class ExperienceMetrics:
  +    """Experience metrics data structure"""
  +    start_rate: float  # Experience start rate
  +    completion_rate: float  # Experience completion rate
  +    avg_completion_time: float  # Average completion time in seconds
  +    popular_experiences: List[Dict[str, Any]]
  +
  +
  +class MetricsAggregator:
  +    """Aggregates and computes metrics from analytics data"""
  +    
  +    def __init__(self, db_session: Session):
  +        self.db = db_session
  +    
  +    def get_engagement_metrics(self, time_range: TimeRange) -> EngagementMetrics:
  +        """Get engagement metrics for the given time range"""
  +        logger.info(f"Computing engagement metrics for {time_range}")
  +        
  +        # Get MAU (Monthly Active Users)
  +        mau = self._get_monthly_active_users(time_range)
  +        
  +        # Get DAU (Daily Active Users)
  +        dau = self._get_daily_active_users(time_range)
  +        
  +        # Get retention rates
  +        retention_d1 = self._get_retention_rate(time_range, days=1)
  +        retention_d7 = self._get_retention_rate(time_range, days=7)
  +        retention_d30 = self._get_retention_rate(time_range, days=30)
  +        
  +        # Get average session duration
  +        avg_session_duration = self._get_avg_session_duration(time_range)
  +        
  +        # Get engagement by module
  +        engagement_by_module = self._get_engagement_by_module(time_range)
  +        
  +        return EngagementMetrics(
  +            mau=mau,
  +            dau=dau,
  +            retention_d1=retention_d1,
  +            retention_d7=retention_d7,
  +            retention_d30=retention_d30,
  +            avg_session_duration=avg_session_duration,
  +            engagement_by_module=engagement_by_module
  +        )
  +    
  +    def get_monetization_metrics(self, time_range: TimeRange) -> MonetizationMetrics:
  +        """Get monetization metrics for the given time range"""
  +        logger.info(f"Computing monetization metrics for {time_range}")
  +        
  +        # Get total revenue
  +        total_revenue = self._get_total_revenue(time_range)
  +        
  +        # Get ARPU (Average Revenue Per User)
  +        arpu = self._get_arpu(time_range)
  +        
  +        # Get ARPPU (Average Revenue Per Paying User)
  +        arppu = self._get_arppu(time_range)
  +        
  +        # Get free to VIP conversion rate
  +        free_to_vip_conversion = self._get_free_to_vip_conversion(time_range)
  +        
  +        # Get LTV (Lifetime Value)
  +        ltv = self._get_ltv(time_range)
  +        
  +        # Get revenue by product
  +        revenue_by_product = self._get_revenue_by_product(time_range)
  +        
  +        return MonetizationMetrics(
  +            total_revenue=total_revenue,
  +            arpu=arpu,
  +            arppu=arppu,
  +            free_to_vip_conversion=free_to_vip_conversion,
  +            ltv=ltv,
  +            revenue_by_product=revenue_by_product
  +        )
  +    
  +    def get_narrative_metrics(self, time_range: TimeRange) -> NarrativeMetrics:
  +        """Get narrative metrics for the given time range"""
  +        logger.info(f"Computing narrative metrics for {time_range}")
  +        
  +        # Get most visited fragments
  +        most_visited_fragments = self._get_most_visited_fragments(time_range)
  +        
  +        # Get completion rate
  +        completion_rate = self._get_narrative_completion_rate(time_range)
  +        
  +        # Get popular decisions
  +        popular_decisions = self._get_popular_decisions(time_range)
  +        
  +        # Get drop-off points
  +        drop_off_points = self._get_drop_off_points(time_range)
  +        
  +        return NarrativeMetrics(
  +            most_visited_fragments=most_visited_fragments,
  +            completion_rate=completion_rate,
  +            popular_decisions=popular_decisions,
  +            drop_off_points=drop_off_points
  +        )
  +    
  +    def get_experience_metrics(self, time_range: TimeRange) -> ExperienceMetrics:
  +        """Get experience metrics for the given time range"""
  +        logger.info(f"Computing experience metrics for {time_range}")
  +        
  +        # Get start rate
  +        start_rate = self._get_experience_start_rate(time_range)
  +        
  +        # Get completion rate
  +        completion_rate = self._get_experience_completion_rate(time_range)
  +        
  +        # Get average completion time
  +        avg_completion_time = self._get_avg_experience_completion_time(time_range)
  +        
  +        # Get popular experiences
  +        popular_experiences = self._get_popular_experiences(time_range)
  +        
  +        return ExperienceMetrics(
  +            start_rate=start_rate,
  +            completion_rate=completion_rate,
  +            avg_completion_time=avg_completion_time,
  +            popular_experiences=popular_experiences
  +        )
  +    
  +    # Private helper methods for metric calculations
  +    
  +    def _get_monthly_active_users(self, time_range: TimeRange) -> int:
  +        """Get monthly active users count"""
  +        from database.models import AnalyticsEvent
  ```
  ----------------------------------------------------------
  üìç L√≠nea 272 | @gemini-code-assist[bot]:
  üí¨ ![high](https://www.gstatic.com/codereviewagent/high-priority.svg)

The retention rate calculation is not standard. It only considers users who were active on the first day of the `time_range` as the cohort. A more standard approach for cohort retention analysis would be to group users by their sign-up date (e.g., all users who signed up in a specific week or month) and then track their activity over subsequent periods. The current implementation could be misleading.
  
  C√≥digo:
  ```
  @@ -0,0 +1,466 @@
  +"""
  +Metrics Aggregator for DianaBot Analytics System
  +Aggregates and computes metrics from raw analytics events
  +"""
  +
  +import logging
  +from datetime import datetime, timedelta
  +from typing import Dict, List, Optional, Any, Tuple
  +from dataclasses import dataclass
  +from sqlalchemy import func, and_, or_, text
  +from sqlalchemy.orm import Session
  +
  +logger = logging.getLogger(__name__)
  +
  +
  +@dataclass
  +class TimeRange:
  +    """Time range for metrics aggregation"""
  +    start_date: datetime
  +    end_date: datetime
  +    
  +    @classmethod
  +    def last_7_days(cls):
  +        """Get time range for last 7 days"""
  +        end_date = datetime.now()
  +        start_date = end_date - timedelta(days=7)
  +        return cls(start_date, end_date)
  +    
  +    @classmethod
  +    def last_30_days(cls):
  +        """Get time range for last 30 days"""
  +        end_date = datetime.now()
  +        start_date = end_date - timedelta(days=30)
  +        return cls(start_date, end_date)
  +    
  +    @classmethod
  +    def this_month(cls):
  +        """Get time range for current month"""
  +        now = datetime.now()
  +        start_date = datetime(now.year, now.month, 1)
  +        end_date = now
  +        return cls(start_date, end_date)
  +
  +
  +@dataclass
  +class EngagementMetrics:
  +    """Engagement metrics data structure"""
  +    mau: int  # Monthly Active Users
  +    dau: int  # Daily Active Users
  +    retention_d1: float  # Day 1 retention
  +    retention_d7: float  # Day 7 retention
  +    retention_d30: float  # Day 30 retention
  +    avg_session_duration: float  # Average session duration in seconds
  +    engagement_by_module: Dict[str, int]  # Engagement count by module
  +
  +
  +@dataclass
  +class MonetizationMetrics:
  +    """Monetization metrics data structure"""
  +    total_revenue: float
  +    arpu: float  # Average Revenue Per User
  +    arppu: float  # Average Revenue Per Paying User
  +    free_to_vip_conversion: float  # Conversion rate from free to VIP
  +    ltv: float  # Lifetime Value
  +    revenue_by_product: Dict[str, float]  # Revenue breakdown by product
  +
  +
  +@dataclass
  +class NarrativeMetrics:
  +    """Narrative metrics data structure"""
  +    most_visited_fragments: List[Dict[str, Any]]
  +    completion_rate: float  # Narrative completion rate
  +    popular_decisions: List[Dict[str, Any]]
  +    drop_off_points: List[Dict[str, Any]]
  +
  +
  +@dataclass
  +class ExperienceMetrics:
  +    """Experience metrics data structure"""
  +    start_rate: float  # Experience start rate
  +    completion_rate: float  # Experience completion rate
  +    avg_completion_time: float  # Average completion time in seconds
  +    popular_experiences: List[Dict[str, Any]]
  +
  +
  +class MetricsAggregator:
  +    """Aggregates and computes metrics from analytics data"""
  +    
  +    def __init__(self, db_session: Session):
  +        self.db = db_session
  +    
  +    def get_engagement_metrics(self, time_range: TimeRange) -> EngagementMetrics:
  +        """Get engagement metrics for the given time range"""
  +        logger.info(f"Computing engagement metrics for {time_range}")
  +        
  +        # Get MAU (Monthly Active Users)
  +        mau = self._get_monthly_active_users(time_range)
  +        
  +        # Get DAU (Daily Active Users)
  +        dau = self._get_daily_active_users(time_range)
  +        
  +        # Get retention rates
  +        retention_d1 = self._get_retention_rate(time_range, days=1)
  +        retention_d7 = self._get_retention_rate(time_range, days=7)
  +        retention_d30 = self._get_retention_rate(time_range, days=30)
  +        
  +        # Get average session duration
  +        avg_session_duration = self._get_avg_session_duration(time_range)
  +        
  +        # Get engagement by module
  +        engagement_by_module = self._get_engagement_by_module(time_range)
  +        
  +        return EngagementMetrics(
  +            mau=mau,
  +            dau=dau,
  +            retention_d1=retention_d1,
  +            retention_d7=retention_d7,
  +            retention_d30=retention_d30,
  +            avg_session_duration=avg_session_duration,
  +            engagement_by_module=engagement_by_module
  +        )
  +    
  +    def get_monetization_metrics(self, time_range: TimeRange) -> MonetizationMetrics:
  +        """Get monetization metrics for the given time range"""
  +        logger.info(f"Computing monetization metrics for {time_range}")
  +        
  +        # Get total revenue
  +        total_revenue = self._get_total_revenue(time_range)
  +        
  +        # Get ARPU (Average Revenue Per User)
  +        arpu = self._get_arpu(time_range)
  +        
  +        # Get ARPPU (Average Revenue Per Paying User)
  +        arppu = self._get_arppu(time_range)
  +        
  +        # Get free to VIP conversion rate
  +        free_to_vip_conversion = self._get_free_to_vip_conversion(time_range)
  +        
  +        # Get LTV (Lifetime Value)
  +        ltv = self._get_ltv(time_range)
  +        
  +        # Get revenue by product
  +        revenue_by_product = self._get_revenue_by_product(time_range)
  +        
  +        return MonetizationMetrics(
  +            total_revenue=total_revenue,
  +            arpu=arpu,
  +            arppu=arppu,
  +            free_to_vip_conversion=free_to_vip_conversion,
  +            ltv=ltv,
  +            revenue_by_product=revenue_by_product
  +        )
  +    
  +    def get_narrative_metrics(self, time_range: TimeRange) -> NarrativeMetrics:
  +        """Get narrative metrics for the given time range"""
  +        logger.info(f"Computing narrative metrics for {time_range}")
  +        
  +        # Get most visited fragments
  +        most_visited_fragments = self._get_most_visited_fragments(time_range)
  +        
  +        # Get completion rate
  +        completion_rate = self._get_narrative_completion_rate(time_range)
  +        
  +        # Get popular decisions
  +        popular_decisions = self._get_popular_decisions(time_range)
  +        
  +        # Get drop-off points
  +        drop_off_points = self._get_drop_off_points(time_range)
  +        
  +        return NarrativeMetrics(
  +            most_visited_fragments=most_visited_fragments,
  +            completion_rate=completion_rate,
  +            popular_decisions=popular_decisions,
  +            drop_off_points=drop_off_points
  +        )
  +    
  +    def get_experience_metrics(self, time_range: TimeRange) -> ExperienceMetrics:
  +        """Get experience metrics for the given time range"""
  +        logger.info(f"Computing experience metrics for {time_range}")
  +        
  +        # Get start rate
  +        start_rate = self._get_experience_start_rate(time_range)
  +        
  +        # Get completion rate
  +        completion_rate = self._get_experience_completion_rate(time_range)
  +        
  +        # Get average completion time
  +        avg_completion_time = self._get_avg_experience_completion_time(time_range)
  +        
  +        # Get popular experiences
  +        popular_experiences = self._get_popular_experiences(time_range)
  +        
  +        return ExperienceMetrics(
  +            start_rate=start_rate,
  +            completion_rate=completion_rate,
  +            avg_completion_time=avg_completion_time,
  +            popular_experiences=popular_experiences
  +        )
  +    
  +    # Private helper methods for metric calculations
  +    
  +    def _get_monthly_active_users(self, time_range: TimeRange) -> int:
  +        """Get monthly active users count"""
  +        from database.models import AnalyticsEvent
  +        
  +        result = self.db.query(func.count(func.distinct(AnalyticsEvent.user_id))).filter(
  +            and_(
  +                AnalyticsEvent.timestamp >= time_range.start_date,
  +                AnalyticsEvent.timestamp <= time_range.end_date
  +            )
  +        ).scalar()
  +        
  +        return result or 0
  +    
  +    def _get_daily_active_users(self, time_range: TimeRange) -> int:
  +        """Get daily active users count (average over time range)"""
  +        from database.models import AnalyticsEvent
  +        
  +        # Get unique users per day and average
  +        daily_counts = self.db.query(
  +            func.date(AnalyticsEvent.timestamp),
  +            func.count(func.distinct(AnalyticsEvent.user_id))
  +        ).filter(
  +            and_(
  +                AnalyticsEvent.timestamp >= time_range.start_date,
  +                AnalyticsEvent.timestamp <= time_range.end_date
  +            )
  +        ).group_by(func.date(AnalyticsEvent.timestamp)).all()
  +        
  +        if not daily_counts:
  +            return 0
  +        
  +        total_dau = sum(count for _, count in daily_counts)
  +        avg_dau = total_dau / len(daily_counts)
  +        
  +        return int(avg_dau)
  +    
  +    def _get_retention_rate(self, time_range: TimeRange, days: int) -> float:
  +        """Get retention rate for given number of days"""
  +        # This is a simplified implementation
  +        # In production, you'd need cohort analysis
  +        from database.models import AnalyticsEvent
  +        
  +        # Get users who had activity in the first day of the range
  +        cohort_start = time_range.start_date
  +        cohort_end = cohort_start + timedelta(days=1)
  +        
  +        cohort_users = self.db.query(func.distinct(AnalyticsEvent.user_id)).filter(
  +            and_(
  +                AnalyticsEvent.timestamp >= cohort_start,
  +                AnalyticsEvent.timestamp <= cohort_end
  +            )
  +        ).all()
  +        
  +        if not cohort_users:
  +            return 0.0
  +        
  +        cohort_user_ids = [user_id for (user_id,) in cohort_users]
  +        
  +        # Check how many of these users were active after N days
  +        retention_start = cohort_start + timedelta(days=days)
  +        retention_end = retention_start + timedelta(days=1)
  +        
  +        retained_users = self.db.query(func.distinct(AnalyticsEvent.user_id)).filter(
  +            and_(
  +                AnalyticsEvent.timestamp >= retention_start,
  +                AnalyticsEvent.timestamp <= retention_end,
  +                AnalyticsEvent.user_id.in_(cohort_user_ids)
  +            )
  +        ).count()
  +        
  +        return retained_users / len(cohort_user_ids) if cohort_user_ids else 0.0
  ```
  ----------------------------------------------------------
  üìç L√≠nea 350 | @gemini-code-assist[bot]:
  üí¨ ![high](https://www.gstatic.com/codereviewagent/high-priority.svg)

The free-to-VIP conversion rate calculation seems incorrect. It divides the total number of VIP users by the total number of all users. A more accurate conversion rate for the given `time_range` would be `(number of users who became VIP in the time range) / (number of active non-VIP users in the time range)`. The current calculation does not reflect the conversion activity within the specified period.
  
  C√≥digo:
  ```
  @@ -0,0 +1,466 @@
  +"""
  +Metrics Aggregator for DianaBot Analytics System
  +Aggregates and computes metrics from raw analytics events
  +"""
  +
  +import logging
  +from datetime import datetime, timedelta
  +from typing import Dict, List, Optional, Any, Tuple
  +from dataclasses import dataclass
  +from sqlalchemy import func, and_, or_, text
  +from sqlalchemy.orm import Session
  +
  +logger = logging.getLogger(__name__)
  +
  +
  +@dataclass
  +class TimeRange:
  +    """Time range for metrics aggregation"""
  +    start_date: datetime
  +    end_date: datetime
  +    
  +    @classmethod
  +    def last_7_days(cls):
  +        """Get time range for last 7 days"""
  +        end_date = datetime.now()
  +        start_date = end_date - timedelta(days=7)
  +        return cls(start_date, end_date)
  +    
  +    @classmethod
  +    def last_30_days(cls):
  +        """Get time range for last 30 days"""
  +        end_date = datetime.now()
  +        start_date = end_date - timedelta(days=30)
  +        return cls(start_date, end_date)
  +    
  +    @classmethod
  +    def this_month(cls):
  +        """Get time range for current month"""
  +        now = datetime.now()
  +        start_date = datetime(now.year, now.month, 1)
  +        end_date = now
  +        return cls(start_date, end_date)
  +
  +
  +@dataclass
  +class EngagementMetrics:
  +    """Engagement metrics data structure"""
  +    mau: int  # Monthly Active Users
  +    dau: int  # Daily Active Users
  +    retention_d1: float  # Day 1 retention
  +    retention_d7: float  # Day 7 retention
  +    retention_d30: float  # Day 30 retention
  +    avg_session_duration: float  # Average session duration in seconds
  +    engagement_by_module: Dict[str, int]  # Engagement count by module
  +
  +
  +@dataclass
  +class MonetizationMetrics:
  +    """Monetization metrics data structure"""
  +    total_revenue: float
  +    arpu: float  # Average Revenue Per User
  +    arppu: float  # Average Revenue Per Paying User
  +    free_to_vip_conversion: float  # Conversion rate from free to VIP
  +    ltv: float  # Lifetime Value
  +    revenue_by_product: Dict[str, float]  # Revenue breakdown by product
  +
  +
  +@dataclass
  +class NarrativeMetrics:
  +    """Narrative metrics data structure"""
  +    most_visited_fragments: List[Dict[str, Any]]
  +    completion_rate: float  # Narrative completion rate
  +    popular_decisions: List[Dict[str, Any]]
  +    drop_off_points: List[Dict[str, Any]]
  +
  +
  +@dataclass
  +class ExperienceMetrics:
  +    """Experience metrics data structure"""
  +    start_rate: float  # Experience start rate
  +    completion_rate: float  # Experience completion rate
  +    avg_completion_time: float  # Average completion time in seconds
  +    popular_experiences: List[Dict[str, Any]]
  +
  +
  +class MetricsAggregator:
  +    """Aggregates and computes metrics from analytics data"""
  +    
  +    def __init__(self, db_session: Session):
  +        self.db = db_session
  +    
  +    def get_engagement_metrics(self, time_range: TimeRange) -> EngagementMetrics:
  +        """Get engagement metrics for the given time range"""
  +        logger.info(f"Computing engagement metrics for {time_range}")
  +        
  +        # Get MAU (Monthly Active Users)
  +        mau = self._get_monthly_active_users(time_range)
  +        
  +        # Get DAU (Daily Active Users)
  +        dau = self._get_daily_active_users(time_range)
  +        
  +        # Get retention rates
  +        retention_d1 = self._get_retention_rate(time_range, days=1)
  +        retention_d7 = self._get_retention_rate(time_range, days=7)
  +        retention_d30 = self._get_retention_rate(time_range, days=30)
  +        
  +        # Get average session duration
  +        avg_session_duration = self._get_avg_session_duration(time_range)
  +        
  +        # Get engagement by module
  +        engagement_by_module = self._get_engagement_by_module(time_range)
  +        
  +        return EngagementMetrics(
  +            mau=mau,
  +            dau=dau,
  +            retention_d1=retention_d1,
  +            retention_d7=retention_d7,
  +            retention_d30=retention_d30,
  +            avg_session_duration=avg_session_duration,
  +            engagement_by_module=engagement_by_module
  +        )
  +    
  +    def get_monetization_metrics(self, time_range: TimeRange) -> MonetizationMetrics:
  +        """Get monetization metrics for the given time range"""
  +        logger.info(f"Computing monetization metrics for {time_range}")
  +        
  +        # Get total revenue
  +        total_revenue = self._get_total_revenue(time_range)
  +        
  +        # Get ARPU (Average Revenue Per User)
  +        arpu = self._get_arpu(time_range)
  +        
  +        # Get ARPPU (Average Revenue Per Paying User)
  +        arppu = self._get_arppu(time_range)
  +        
  +        # Get free to VIP conversion rate
  +        free_to_vip_conversion = self._get_free_to_vip_conversion(time_range)
  +        
  +        # Get LTV (Lifetime Value)
  +        ltv = self._get_ltv(time_range)
  +        
  +        # Get revenue by product
  +        revenue_by_product = self._get_revenue_by_product(time_range)
  +        
  +        return MonetizationMetrics(
  +            total_revenue=total_revenue,
  +            arpu=arpu,
  +            arppu=arppu,
  +            free_to_vip_conversion=free_to_vip_conversion,
  +            ltv=ltv,
  +            revenue_by_product=revenue_by_product
  +        )
  +    
  +    def get_narrative_metrics(self, time_range: TimeRange) -> NarrativeMetrics:
  +        """Get narrative metrics for the given time range"""
  +        logger.info(f"Computing narrative metrics for {time_range}")
  +        
  +        # Get most visited fragments
  +        most_visited_fragments = self._get_most_visited_fragments(time_range)
  +        
  +        # Get completion rate
  +        completion_rate = self._get_narrative_completion_rate(time_range)
  +        
  +        # Get popular decisions
  +        popular_decisions = self._get_popular_decisions(time_range)
  +        
  +        # Get drop-off points
  +        drop_off_points = self._get_drop_off_points(time_range)
  +        
  +        return NarrativeMetrics(
  +            most_visited_fragments=most_visited_fragments,
  +            completion_rate=completion_rate,
  +            popular_decisions=popular_decisions,
  +            drop_off_points=drop_off_points
  +        )
  +    
  +    def get_experience_metrics(self, time_range: TimeRange) -> ExperienceMetrics:
  +        """Get experience metrics for the given time range"""
  +        logger.info(f"Computing experience metrics for {time_range}")
  +        
  +        # Get start rate
  +        start_rate = self._get_experience_start_rate(time_range)
  +        
  +        # Get completion rate
  +        completion_rate = self._get_experience_completion_rate(time_range)
  +        
  +        # Get average completion time
  +        avg_completion_time = self._get_avg_experience_completion_time(time_range)
  +        
  +        # Get popular experiences
  +        popular_experiences = self._get_popular_experiences(time_range)
  +        
  +        return ExperienceMetrics(
  +            start_rate=start_rate,
  +            completion_rate=completion_rate,
  +            avg_completion_time=avg_completion_time,
  +            popular_experiences=popular_experiences
  +        )
  +    
  +    # Private helper methods for metric calculations
  +    
  +    def _get_monthly_active_users(self, time_range: TimeRange) -> int:
  +        """Get monthly active users count"""
  +        from database.models import AnalyticsEvent
  +        
  +        result = self.db.query(func.count(func.distinct(AnalyticsEvent.user_id))).filter(
  +            and_(
  +                AnalyticsEvent.timestamp >= time_range.start_date,
  +                AnalyticsEvent.timestamp <= time_range.end_date
  +            )
  +        ).scalar()
  +        
  +        return result or 0
  +    
  +    def _get_daily_active_users(self, time_range: TimeRange) -> int:
  +        """Get daily active users count (average over time range)"""
  +        from database.models import AnalyticsEvent
  +        
  +        # Get unique users per day and average
  +        daily_counts = self.db.query(
  +            func.date(AnalyticsEvent.timestamp),
  +            func.count(func.distinct(AnalyticsEvent.user_id))
  +        ).filter(
  +            and_(
  +                AnalyticsEvent.timestamp >= time_range.start_date,
  +                AnalyticsEvent.timestamp <= time_range.end_date
  +            )
  +        ).group_by(func.date(AnalyticsEvent.timestamp)).all()
  +        
  +        if not daily_counts:
  +            return 0
  +        
  +        total_dau = sum(count for _, count in daily_counts)
  +        avg_dau = total_dau / len(daily_counts)
  +        
  +        return int(avg_dau)
  +    
  +    def _get_retention_rate(self, time_range: TimeRange, days: int) -> float:
  +        """Get retention rate for given number of days"""
  +        # This is a simplified implementation
  +        # In production, you'd need cohort analysis
  +        from database.models import AnalyticsEvent
  +        
  +        # Get users who had activity in the first day of the range
  +        cohort_start = time_range.start_date
  +        cohort_end = cohort_start + timedelta(days=1)
  +        
  +        cohort_users = self.db.query(func.distinct(AnalyticsEvent.user_id)).filter(
  +            and_(
  +                AnalyticsEvent.timestamp >= cohort_start,
  +                AnalyticsEvent.timestamp <= cohort_end
  +            )
  +        ).all()
  +        
  +        if not cohort_users:
  +            return 0.0
  +        
  +        cohort_user_ids = [user_id for (user_id,) in cohort_users]
  +        
  +        # Check how many of these users were active after N days
  +        retention_start = cohort_start + timedelta(days=days)
  +        retention_end = retention_start + timedelta(days=1)
  +        
  +        retained_users = self.db.query(func.distinct(AnalyticsEvent.user_id)).filter(
  +            and_(
  +                AnalyticsEvent.timestamp >= retention_start,
  +                AnalyticsEvent.timestamp <= retention_end,
  +                AnalyticsEvent.user_id.in_(cohort_user_ids)
  +            )
  +        ).count()
  +        
  +        return retained_users / len(cohort_user_ids) if cohort_user_ids else 0.0
  +    
  +    def _get_avg_session_duration(self, time_range: TimeRange) -> float:
  +        """Get average session duration in seconds"""
  +        # This would require session tracking in the events
  +        # For now, return a placeholder value
  +        return 300.0  # 5 minutes average
  +    
  +    def _get_engagement_by_module(self, time_range: TimeRange) -> Dict[str, int]:
  +        """Get engagement count by module"""
  +        from database.models import AnalyticsEvent
  +        
  +        # Group events by event_type (which corresponds to modules)
  +        module_engagement = self.db.query(
  +            AnalyticsEvent.event_type,
  +            func.count(AnalyticsEvent.id)
  +        ).filter(
  +            and_(
  +                AnalyticsEvent.timestamp >= time_range.start_date,
  +                AnalyticsEvent.timestamp <= time_range.end_date
  +            )
  +        ).group_by(AnalyticsEvent.event_type).all()
  +        
  +        return {module: count for module, count in module_engagement}
  +    
  +    def _get_total_revenue(self, time_range: TimeRange) -> float:
  +        """Get total revenue for time range"""
  +        from database.models import Transaction
  +        
  +        result = self.db.query(func.sum(Transaction.amount)).filter(
  +            and_(
  +                Transaction.created_at >= time_range.start_date,
  +                Transaction.created_at <= time_range.end_date,
  +                Transaction.status == 'completed'
  +            )
  +        ).scalar()
  +        
  +        return float(result or 0)
  +    
  +    def _get_arpu(self, time_range: TimeRange) -> float:
  +        """Get Average Revenue Per User"""
  +        total_revenue = self._get_total_revenue(time_range)
  +        total_users = self._get_monthly_active_users(time_range)
  +        
  +        return total_revenue / total_users if total_users > 0 else 0.0
  +    
  +    def _get_arppu(self, time_range: TimeRange) -> float:
  +        """Get Average Revenue Per Paying User"""
  +        from database.models import Transaction
  +        
  +        total_revenue = self._get_total_revenue(time_range)
  +        
  +        # Count unique paying users
  +        paying_users = self.db.query(func.count(func.distinct(Transaction.user_id))).filter(
  +            and_(
  +                Transaction.created_at >= time_range.start_date,
  +                Transaction.created_at <= time_range.end_date,
  +                Transaction.status == 'completed'
  +            )
  +        ).scalar()
  +        
  +        return total_revenue / paying_users if paying_users > 0 else 0.0
  +    
  +    def _get_free_to_vip_conversion(self, time_range: TimeRange) -> float:
  +        """Get free to VIP conversion rate"""
  +        from database.models import User, Subscription
  +        
  +        # Get total users
  +        total_users = self.db.query(func.count(User.id)).scalar()
  +        
  +        # Get VIP users
  +        vip_users = self.db.query(func.count(func.distinct(Subscription.user_id))).filter(
  +            and_(
  +                Subscription.status == 'active',
  +                Subscription.created_at <= time_range.end_date
  +            )
  +        ).scalar()
  +        
  +        return vip_users / total_users if total_users > 0 else 0.0
  ```
  ----------------------------------------------------------
  üìç L√≠nea 356 | @gemini-code-assist[bot]:
  üí¨ ![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)

The `avg_customer_lifetime` is hardcoded to 6 months. This makes the LTV calculation inflexible and potentially inaccurate. This value should be calculated based on historical data or at least be a configurable parameter rather than a magic number.
  
  C√≥digo:
  ```
  @@ -0,0 +1,466 @@
  +"""
  +Metrics Aggregator for DianaBot Analytics System
  +Aggregates and computes metrics from raw analytics events
  +"""
  +
  +import logging
  +from datetime import datetime, timedelta
  +from typing import Dict, List, Optional, Any, Tuple
  +from dataclasses import dataclass
  +from sqlalchemy import func, and_, or_, text
  +from sqlalchemy.orm import Session
  +
  +logger = logging.getLogger(__name__)
  +
  +
  +@dataclass
  +class TimeRange:
  +    """Time range for metrics aggregation"""
  +    start_date: datetime
  +    end_date: datetime
  +    
  +    @classmethod
  +    def last_7_days(cls):
  +        """Get time range for last 7 days"""
  +        end_date = datetime.now()
  +        start_date = end_date - timedelta(days=7)
  +        return cls(start_date, end_date)
  +    
  +    @classmethod
  +    def last_30_days(cls):
  +        """Get time range for last 30 days"""
  +        end_date = datetime.now()
  +        start_date = end_date - timedelta(days=30)
  +        return cls(start_date, end_date)
  +    
  +    @classmethod
  +    def this_month(cls):
  +        """Get time range for current month"""
  +        now = datetime.now()
  +        start_date = datetime(now.year, now.month, 1)
  +        end_date = now
  +        return cls(start_date, end_date)
  +
  +
  +@dataclass
  +class EngagementMetrics:
  +    """Engagement metrics data structure"""
  +    mau: int  # Monthly Active Users
  +    dau: int  # Daily Active Users
  +    retention_d1: float  # Day 1 retention
  +    retention_d7: float  # Day 7 retention
  +    retention_d30: float  # Day 30 retention
  +    avg_session_duration: float  # Average session duration in seconds
  +    engagement_by_module: Dict[str, int]  # Engagement count by module
  +
  +
  +@dataclass
  +class MonetizationMetrics:
  +    """Monetization metrics data structure"""
  +    total_revenue: float
  +    arpu: float  # Average Revenue Per User
  +    arppu: float  # Average Revenue Per Paying User
  +    free_to_vip_conversion: float  # Conversion rate from free to VIP
  +    ltv: float  # Lifetime Value
  +    revenue_by_product: Dict[str, float]  # Revenue breakdown by product
  +
  +
  +@dataclass
  +class NarrativeMetrics:
  +    """Narrative metrics data structure"""
  +    most_visited_fragments: List[Dict[str, Any]]
  +    completion_rate: float  # Narrative completion rate
  +    popular_decisions: List[Dict[str, Any]]
  +    drop_off_points: List[Dict[str, Any]]
  +
  +
  +@dataclass
  +class ExperienceMetrics:
  +    """Experience metrics data structure"""
  +    start_rate: float  # Experience start rate
  +    completion_rate: float  # Experience completion rate
  +    avg_completion_time: float  # Average completion time in seconds
  +    popular_experiences: List[Dict[str, Any]]
  +
  +
  +class MetricsAggregator:
  +    """Aggregates and computes metrics from analytics data"""
  +    
  +    def __init__(self, db_session: Session):
  +        self.db = db_session
  +    
  +    def get_engagement_metrics(self, time_range: TimeRange) -> EngagementMetrics:
  +        """Get engagement metrics for the given time range"""
  +        logger.info(f"Computing engagement metrics for {time_range}")
  +        
  +        # Get MAU (Monthly Active Users)
  +        mau = self._get_monthly_active_users(time_range)
  +        
  +        # Get DAU (Daily Active Users)
  +        dau = self._get_daily_active_users(time_range)
  +        
  +        # Get retention rates
  +        retention_d1 = self._get_retention_rate(time_range, days=1)
  +        retention_d7 = self._get_retention_rate(time_range, days=7)
  +        retention_d30 = self._get_retention_rate(time_range, days=30)
  +        
  +        # Get average session duration
  +        avg_session_duration = self._get_avg_session_duration(time_range)
  +        
  +        # Get engagement by module
  +        engagement_by_module = self._get_engagement_by_module(time_range)
  +        
  +        return EngagementMetrics(
  +            mau=mau,
  +            dau=dau,
  +            retention_d1=retention_d1,
  +            retention_d7=retention_d7,
  +            retention_d30=retention_d30,
  +            avg_session_duration=avg_session_duration,
  +            engagement_by_module=engagement_by_module
  +        )
  +    
  +    def get_monetization_metrics(self, time_range: TimeRange) -> MonetizationMetrics:
  +        """Get monetization metrics for the given time range"""
  +        logger.info(f"Computing monetization metrics for {time_range}")
  +        
  +        # Get total revenue
  +        total_revenue = self._get_total_revenue(time_range)
  +        
  +        # Get ARPU (Average Revenue Per User)
  +        arpu = self._get_arpu(time_range)
  +        
  +        # Get ARPPU (Average Revenue Per Paying User)
  +        arppu = self._get_arppu(time_range)
  +        
  +        # Get free to VIP conversion rate
  +        free_to_vip_conversion = self._get_free_to_vip_conversion(time_range)
  +        
  +        # Get LTV (Lifetime Value)
  +        ltv = self._get_ltv(time_range)
  +        
  +        # Get revenue by product
  +        revenue_by_product = self._get_revenue_by_product(time_range)
  +        
  +        return MonetizationMetrics(
  +            total_revenue=total_revenue,
  +            arpu=arpu,
  +            arppu=arppu,
  +            free_to_vip_conversion=free_to_vip_conversion,
  +            ltv=ltv,
  +            revenue_by_product=revenue_by_product
  +        )
  +    
  +    def get_narrative_metrics(self, time_range: TimeRange) -> NarrativeMetrics:
  +        """Get narrative metrics for the given time range"""
  +        logger.info(f"Computing narrative metrics for {time_range}")
  +        
  +        # Get most visited fragments
  +        most_visited_fragments = self._get_most_visited_fragments(time_range)
  +        
  +        # Get completion rate
  +        completion_rate = self._get_narrative_completion_rate(time_range)
  +        
  +        # Get popular decisions
  +        popular_decisions = self._get_popular_decisions(time_range)
  +        
  +        # Get drop-off points
  +        drop_off_points = self._get_drop_off_points(time_range)
  +        
  +        return NarrativeMetrics(
  +            most_visited_fragments=most_visited_fragments,
  +            completion_rate=completion_rate,
  +            popular_decisions=popular_decisions,
  +            drop_off_points=drop_off_points
  +        )
  +    
  +    def get_experience_metrics(self, time_range: TimeRange) -> ExperienceMetrics:
  +        """Get experience metrics for the given time range"""
  +        logger.info(f"Computing experience metrics for {time_range}")
  +        
  +        # Get start rate
  +        start_rate = self._get_experience_start_rate(time_range)
  +        
  +        # Get completion rate
  +        completion_rate = self._get_experience_completion_rate(time_range)
  +        
  +        # Get average completion time
  +        avg_completion_time = self._get_avg_experience_completion_time(time_range)
  +        
  +        # Get popular experiences
  +        popular_experiences = self._get_popular_experiences(time_range)
  +        
  +        return ExperienceMetrics(
  +            start_rate=start_rate,
  +            completion_rate=completion_rate,
  +            avg_completion_time=avg_completion_time,
  +            popular_experiences=popular_experiences
  +        )
  +    
  +    # Private helper methods for metric calculations
  +    
  +    def _get_monthly_active_users(self, time_range: TimeRange) -> int:
  +        """Get monthly active users count"""
  +        from database.models import AnalyticsEvent
  +        
  +        result = self.db.query(func.count(func.distinct(AnalyticsEvent.user_id))).filter(
  +            and_(
  +                AnalyticsEvent.timestamp >= time_range.start_date,
  +                AnalyticsEvent.timestamp <= time_range.end_date
  +            )
  +        ).scalar()
  +        
  +        return result or 0
  +    
  +    def _get_daily_active_users(self, time_range: TimeRange) -> int:
  +        """Get daily active users count (average over time range)"""
  +        from database.models import AnalyticsEvent
  +        
  +        # Get unique users per day and average
  +        daily_counts = self.db.query(
  +            func.date(AnalyticsEvent.timestamp),
  +            func.count(func.distinct(AnalyticsEvent.user_id))
  +        ).filter(
  +            and_(
  +                AnalyticsEvent.timestamp >= time_range.start_date,
  +                AnalyticsEvent.timestamp <= time_range.end_date
  +            )
  +        ).group_by(func.date(AnalyticsEvent.timestamp)).all()
  +        
  +        if not daily_counts:
  +            return 0
  +        
  +        total_dau = sum(count for _, count in daily_counts)
  +        avg_dau = total_dau / len(daily_counts)
  +        
  +        return int(avg_dau)
  +    
  +    def _get_retention_rate(self, time_range: TimeRange, days: int) -> float:
  +        """Get retention rate for given number of days"""
  +        # This is a simplified implementation
  +        # In production, you'd need cohort analysis
  +        from database.models import AnalyticsEvent
  +        
  +        # Get users who had activity in the first day of the range
  +        cohort_start = time_range.start_date
  +        cohort_end = cohort_start + timedelta(days=1)
  +        
  +        cohort_users = self.db.query(func.distinct(AnalyticsEvent.user_id)).filter(
  +            and_(
  +                AnalyticsEvent.timestamp >= cohort_start,
  +                AnalyticsEvent.timestamp <= cohort_end
  +            )
  +        ).all()
  +        
  +        if not cohort_users:
  +            return 0.0
  +        
  +        cohort_user_ids = [user_id for (user_id,) in cohort_users]
  +        
  +        # Check how many of these users were active after N days
  +        retention_start = cohort_start + timedelta(days=days)
  +        retention_end = retention_start + timedelta(days=1)
  +        
  +        retained_users = self.db.query(func.distinct(AnalyticsEvent.user_id)).filter(
  +            and_(
  +                AnalyticsEvent.timestamp >= retention_start,
  +                AnalyticsEvent.timestamp <= retention_end,
  +                AnalyticsEvent.user_id.in_(cohort_user_ids)
  +            )
  +        ).count()
  +        
  +        return retained_users / len(cohort_user_ids) if cohort_user_ids else 0.0
  +    
  +    def _get_avg_session_duration(self, time_range: TimeRange) -> float:
  +        """Get average session duration in seconds"""
  +        # This would require session tracking in the events
  +        # For now, return a placeholder value
  +        return 300.0  # 5 minutes average
  +    
  +    def _get_engagement_by_module(self, time_range: TimeRange) -> Dict[str, int]:
  +        """Get engagement count by module"""
  +        from database.models import AnalyticsEvent
  +        
  +        # Group events by event_type (which corresponds to modules)
  +        module_engagement = self.db.query(
  +            AnalyticsEvent.event_type,
  +            func.count(AnalyticsEvent.id)
  +        ).filter(
  +            and_(
  +                AnalyticsEvent.timestamp >= time_range.start_date,
  +                AnalyticsEvent.timestamp <= time_range.end_date
  +            )
  +        ).group_by(AnalyticsEvent.event_type).all()
  +        
  +        return {module: count for module, count in module_engagement}
  +    
  +    def _get_total_revenue(self, time_range: TimeRange) -> float:
  +        """Get total revenue for time range"""
  +        from database.models import Transaction
  +        
  +        result = self.db.query(func.sum(Transaction.amount)).filter(
  +            and_(
  +                Transaction.created_at >= time_range.start_date,
  +                Transaction.created_at <= time_range.end_date,
  +                Transaction.status == 'completed'
  +            )
  +        ).scalar()
  +        
  +        return float(result or 0)
  +    
  +    def _get_arpu(self, time_range: TimeRange) -> float:
  +        """Get Average Revenue Per User"""
  +        total_revenue = self._get_total_revenue(time_range)
  +        total_users = self._get_monthly_active_users(time_range)
  +        
  +        return total_revenue / total_users if total_users > 0 else 0.0
  +    
  +    def _get_arppu(self, time_range: TimeRange) -> float:
  +        """Get Average Revenue Per Paying User"""
  +        from database.models import Transaction
  +        
  +        total_revenue = self._get_total_revenue(time_range)
  +        
  +        # Count unique paying users
  +        paying_users = self.db.query(func.count(func.distinct(Transaction.user_id))).filter(
  +            and_(
  +                Transaction.created_at >= time_range.start_date,
  +                Transaction.created_at <= time_range.end_date,
  +                Transaction.status == 'completed'
  +            )
  +        ).scalar()
  +        
  +        return total_revenue / paying_users if paying_users > 0 else 0.0
  +    
  +    def _get_free_to_vip_conversion(self, time_range: TimeRange) -> float:
  +        """Get free to VIP conversion rate"""
  +        from database.models import User, Subscription
  +        
  +        # Get total users
  +        total_users = self.db.query(func.count(User.id)).scalar()
  +        
  +        # Get VIP users
  +        vip_users = self.db.query(func.count(func.distinct(Subscription.user_id))).filter(
  +            and_(
  +                Subscription.status == 'active',
  +                Subscription.created_at <= time_range.end_date
  +            )
  +        ).scalar()
  +        
  +        return vip_users / total_users if total_users > 0 else 0.0
  +    
  +    def _get_ltv(self, time_range: TimeRange) -> float:
  +        """Get Lifetime Value (simplified calculation)"""
  +        # Simplified LTV calculation
  +        arpu = self._get_arpu(time_range)
  +        avg_customer_lifetime = 6  # months (placeholder)
  ```
  ----------------------------------------------------------

üìÑ ARCHIVO: modules/analytics/alerts.py
============================================================
  üìç L√≠nea 168 | @gemini-code-assist[bot]:
  üí¨ ![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)

Alert configurations are hardcoded within the `_load_alert_configs` method. This makes them difficult to manage and update without code changes. Consider moving these configurations to a separate configuration file (e.g., YAML, JSON) or a database table to allow for easier management and dynamic updates.
  
  C√≥digo:
  ```
  @@ -0,0 +1,465 @@
  +"""
  +Alert System for DianaBot Analytics System
  +Detects anomalies and sends notifications to admins
  +"""
  +
  +import logging
  +import asyncio
  +from datetime import datetime, timedelta
  +from typing import Dict, List, Optional, Any, Tuple
  +from dataclasses import dataclass
  +from sqlalchemy import func, and_, or_, text
  +from sqlalchemy.orm import Session
  +
  +logger = logging.getLogger(__name__)
  +
  +
  +@dataclass
  +class Alert:
  +    """Alert data structure"""
  +    alert_id: str
  +    alert_type: str
  +    severity: str  # low, medium, high, critical
  +    title: str
  +    description: str
  +    data: Dict[str, Any]
  +    detected_at: datetime
  +    acknowledged: bool = False
  +    acknowledged_by: Optional[str] = None
  +    acknowledged_at: Optional[datetime] = None
  +
  +
  +@dataclass
  +class AlertConfig:
  +    """Alert configuration"""
  +    alert_type: str
  +    enabled: bool
  +    threshold: float
  +    comparison: str  # "greater_than", "less_than", "equal"
  +    time_window_hours: int
  +    notification_channels: List[str]  # "telegram", "email", "dashboard"
  +
  +
  +class AlertSystem:
  +    """Detects anomalies and sends notifications to admins"""
  +    
  +    def __init__(self, db_session: Session):
  +        self.db = db_session
  +        self.alert_configs = self._load_alert_configs()
  +    
  +    def check_anomalies(self) -> List[Alert]:
  +        """Check for all types of anomalies and return alerts"""
  +        logger.info("Checking for anomalies across all systems")
  +        
  +        alerts = []
  +        
  +        # Check engagement anomalies
  +        engagement_alerts = self._check_engagement_anomalies()
  +        alerts.extend(engagement_alerts)
  +        
  +        # Check monetization anomalies
  +        monetization_alerts = self._check_monetization_anomalies()
  +        alerts.extend(monetization_alerts)
  +        
  +        # Check technical anomalies
  +        technical_alerts = self._check_technical_anomalies()
  +        alerts.extend(technical_alerts)
  +        
  +        # Check content anomalies
  +        content_alerts = self._check_content_anomalies()
  +        alerts.extend(content_alerts)
  +        
  +        # Check user behavior anomalies
  +        user_alerts = self._check_user_anomalies()
  +        alerts.extend(user_alerts)
  +        
  +        # Send notifications for critical alerts
  +        for alert in alerts:
  +            if alert.severity in ['high', 'critical']:
  +                self._notify_admin(alert)
  +        
  +        return alerts
  +    
  +    def notify_admin(self, alert: Alert) -> bool:
  +        """Send notification to admin via configured channels"""
  +        logger.info(f"Sending admin notification for alert: {alert.alert_type}")
  +        
  +        config = self.alert_configs.get(alert.alert_type)
  +        if not config or not config.enabled:
  +            logger.warning(f"Alert type {alert.alert_type} is disabled or not configured")
  +            return False
  +        
  +        success = True
  +        
  +        # Send via Telegram
  +        if 'telegram' in config.notification_channels:
  +            success = success and self._send_telegram_notification(alert)
  +        
  +        # Send via Email
  +        if 'email' in config.notification_channels:
  +            success = success and self._send_email_notification(alert)
  +        
  +        # Send to Dashboard
  +        if 'dashboard' in config.notification_channels:
  +            success = success and self._send_dashboard_notification(alert)
  +        
  +        return success
  +    
  +    def get_alert_config(self, alert_type: str) -> Optional[AlertConfig]:
  +        """Get configuration for specific alert type"""
  +        return self.alert_configs.get(alert_type)
  +    
  +    def update_alert_config(self, alert_type: str, config: AlertConfig) -> bool:
  +        """Update configuration for alert type"""
  +        self.alert_configs[alert_type] = config
  +        return True
  +    
  +    def acknowledge_alert(self, alert_id: str, acknowledged_by: str) -> bool:
  +        """Mark alert as acknowledged"""
  +        # In production, this would update the alert in database
  +        logger.info(f"Alert {alert_id} acknowledged by {acknowledged_by}")
  +        return True
  +    
  +    # Private helper methods
  +    
  +    def _load_alert_configs(self) -> Dict[str, AlertConfig]:
  +        """Load alert configurations"""
  +        return {
  +            'engagement_drop': AlertConfig(
  +                alert_type='engagement_drop',
  +                enabled=True,
  +                threshold=0.2,  # 20% drop
  +                comparison='less_than',
  +                time_window_hours=24,
  +                notification_channels=['telegram', 'dashboard']
  +            ),
  +            'conversion_drop': AlertConfig(
  +                alert_type='conversion_drop',
  +                enabled=True,
  +                threshold=0.15,  # 15% drop
  +                comparison='less_than',
  +                time_window_hours=24,
  +                notification_channels=['telegram', 'email', 'dashboard']
  +            ),
  +            'technical_errors': AlertConfig(
  +                alert_type='technical_errors',
  +                enabled=True,
  +                threshold=10,  # 10 errors
  +                comparison='greater_than',
  +                time_window_hours=1,
  +                notification_channels=['telegram', 'email']
  +            ),
  +            'high_value_user_risk': AlertConfig(
  +                alert_type='high_value_user_risk',
  +                enabled=True,
  +                threshold=0.7,  # 70% probability
  +                comparison='greater_than',
  +                time_window_hours=24,
  +                notification_channels=['dashboard']
  +            ),
  +            'besitos_economy_anomaly': AlertConfig(
  +                alert_type='besitos_economy_anomaly',
  +                enabled=True,
  +                threshold=0.3,  # 30% deviation
  +                comparison='greater_than',
  +                time_window_hours=24,
  +                notification_channels=['dashboard']
  +            )
  +        }
  ```
  ----------------------------------------------------------
  üìç L√≠nea 346 | @gemini-code-assist[bot]:
  üí¨ ![critical](https://www.gstatic.com/codereviewagent/critical.svg)

The method `_notify_admin` calls `self.notify_admin(alert)`, which creates an infinite recursion and will result in a `RecursionError`. This method seems intended to be a helper for `check_anomalies` to dispatch notifications, but it calls the public method. The call to `self.notify_admin(alert)` should be removed or the logic re-structured to avoid this recursion.
  
  C√≥digo:
  ```
  @@ -0,0 +1,465 @@
  +"""
  +Alert System for DianaBot Analytics System
  +Detects anomalies and sends notifications to admins
  +"""
  +
  +import logging
  +import asyncio
  +from datetime import datetime, timedelta
  +from typing import Dict, List, Optional, Any, Tuple
  +from dataclasses import dataclass
  +from sqlalchemy import func, and_, or_, text
  +from sqlalchemy.orm import Session
  +
  +logger = logging.getLogger(__name__)
  +
  +
  +@dataclass
  +class Alert:
  +    """Alert data structure"""
  +    alert_id: str
  +    alert_type: str
  +    severity: str  # low, medium, high, critical
  +    title: str
  +    description: str
  +    data: Dict[str, Any]
  +    detected_at: datetime
  +    acknowledged: bool = False
  +    acknowledged_by: Optional[str] = None
  +    acknowledged_at: Optional[datetime] = None
  +
  +
  +@dataclass
  +class AlertConfig:
  +    """Alert configuration"""
  +    alert_type: str
  +    enabled: bool
  +    threshold: float
  +    comparison: str  # "greater_than", "less_than", "equal"
  +    time_window_hours: int
  +    notification_channels: List[str]  # "telegram", "email", "dashboard"
  +
  +
  +class AlertSystem:
  +    """Detects anomalies and sends notifications to admins"""
  +    
  +    def __init__(self, db_session: Session):
  +        self.db = db_session
  +        self.alert_configs = self._load_alert_configs()
  +    
  +    def check_anomalies(self) -> List[Alert]:
  +        """Check for all types of anomalies and return alerts"""
  +        logger.info("Checking for anomalies across all systems")
  +        
  +        alerts = []
  +        
  +        # Check engagement anomalies
  +        engagement_alerts = self._check_engagement_anomalies()
  +        alerts.extend(engagement_alerts)
  +        
  +        # Check monetization anomalies
  +        monetization_alerts = self._check_monetization_anomalies()
  +        alerts.extend(monetization_alerts)
  +        
  +        # Check technical anomalies
  +        technical_alerts = self._check_technical_anomalies()
  +        alerts.extend(technical_alerts)
  +        
  +        # Check content anomalies
  +        content_alerts = self._check_content_anomalies()
  +        alerts.extend(content_alerts)
  +        
  +        # Check user behavior anomalies
  +        user_alerts = self._check_user_anomalies()
  +        alerts.extend(user_alerts)
  +        
  +        # Send notifications for critical alerts
  +        for alert in alerts:
  +            if alert.severity in ['high', 'critical']:
  +                self._notify_admin(alert)
  +        
  +        return alerts
  +    
  +    def notify_admin(self, alert: Alert) -> bool:
  +        """Send notification to admin via configured channels"""
  +        logger.info(f"Sending admin notification for alert: {alert.alert_type}")
  +        
  +        config = self.alert_configs.get(alert.alert_type)
  +        if not config or not config.enabled:
  +            logger.warning(f"Alert type {alert.alert_type} is disabled or not configured")
  +            return False
  +        
  +        success = True
  +        
  +        # Send via Telegram
  +        if 'telegram' in config.notification_channels:
  +            success = success and self._send_telegram_notification(alert)
  +        
  +        # Send via Email
  +        if 'email' in config.notification_channels:
  +            success = success and self._send_email_notification(alert)
  +        
  +        # Send to Dashboard
  +        if 'dashboard' in config.notification_channels:
  +            success = success and self._send_dashboard_notification(alert)
  +        
  +        return success
  +    
  +    def get_alert_config(self, alert_type: str) -> Optional[AlertConfig]:
  +        """Get configuration for specific alert type"""
  +        return self.alert_configs.get(alert_type)
  +    
  +    def update_alert_config(self, alert_type: str, config: AlertConfig) -> bool:
  +        """Update configuration for alert type"""
  +        self.alert_configs[alert_type] = config
  +        return True
  +    
  +    def acknowledge_alert(self, alert_id: str, acknowledged_by: str) -> bool:
  +        """Mark alert as acknowledged"""
  +        # In production, this would update the alert in database
  +        logger.info(f"Alert {alert_id} acknowledged by {acknowledged_by}")
  +        return True
  +    
  +    # Private helper methods
  +    
  +    def _load_alert_configs(self) -> Dict[str, AlertConfig]:
  +        """Load alert configurations"""
  +        return {
  +            'engagement_drop': AlertConfig(
  +                alert_type='engagement_drop',
  +                enabled=True,
  +                threshold=0.2,  # 20% drop
  +                comparison='less_than',
  +                time_window_hours=24,
  +                notification_channels=['telegram', 'dashboard']
  +            ),
  +            'conversion_drop': AlertConfig(
  +                alert_type='conversion_drop',
  +                enabled=True,
  +                threshold=0.15,  # 15% drop
  +                comparison='less_than',
  +                time_window_hours=24,
  +                notification_channels=['telegram', 'email', 'dashboard']
  +            ),
  +            'technical_errors': AlertConfig(
  +                alert_type='technical_errors',
  +                enabled=True,
  +                threshold=10,  # 10 errors
  +                comparison='greater_than',
  +                time_window_hours=1,
  +                notification_channels=['telegram', 'email']
  +            ),
  +            'high_value_user_risk': AlertConfig(
  +                alert_type='high_value_user_risk',
  +                enabled=True,
  +                threshold=0.7,  # 70% probability
  +                comparison='greater_than',
  +                time_window_hours=24,
  +                notification_channels=['dashboard']
  +            ),
  +            'besitos_economy_anomaly': AlertConfig(
  +                alert_type='besitos_economy_anomaly',
  +                enabled=True,
  +                threshold=0.3,  # 30% deviation
  +                comparison='greater_than',
  +                time_window_hours=24,
  +                notification_channels=['dashboard']
  +            )
  +        }
  +    
  +    def _check_engagement_anomalies(self) -> List[Alert]:
  +        """Check for engagement anomalies"""
  +        alerts = []
  +        
  +        # Check DAU drop
  +        current_dau = self._get_current_dau()
  +        previous_dau = self._get_previous_dau()
  +        
  +        if previous_dau > 0:
  +            drop_percentage = (previous_dau - current_dau) / previous_dau
  +            
  +            if drop_percentage > 0.2:  # More than 20% drop
  +                alerts.append(Alert(
  +                    alert_id=f"engagement_drop_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
  +                    alert_type='engagement_drop',
  +                    severity='high',
  +                    title="Significant Drop in Daily Active Users",
  +                    description=f"DAU dropped by {drop_percentage:.1%} compared to previous period",
  +                    data={
  +                        'current_dau': current_dau,
  +                        'previous_dau': previous_dau,
  +                        'drop_percentage': drop_percentage
  +                    },
  +                    detected_at=datetime.now()
  +                ))
  +        
  +        # Check session duration drop
  +        current_session_duration = self._get_current_session_duration()
  +        previous_session_duration = self._get_previous_session_duration()
  +        
  +        if previous_session_duration > 0:
  +            drop_percentage = (previous_session_duration - current_session_duration) / previous_session_duration
  +            
  +            if drop_percentage > 0.3:  # More than 30% drop
  +                alerts.append(Alert(
  +                    alert_id=f"session_duration_drop_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
  +                    alert_type='engagement_drop',
  +                    severity='medium',
  +                    title="Drop in Average Session Duration",
  +                    description=f"Average session duration dropped by {drop_percentage:.1%}",
  +                    data={
  +                        'current_duration': current_session_duration,
  +                        'previous_duration': previous_session_duration,
  +                        'drop_percentage': drop_percentage
  +                    },
  +                    detected_at=datetime.now()
  +                ))
  +        
  +        return alerts
  +    
  +    def _check_monetization_anomalies(self) -> List[Alert]:
  +        """Check for monetization anomalies"""
  +        alerts = []
  +        
  +        # Check conversion rate drop
  +        current_conversion = self._get_current_conversion_rate()
  +        previous_conversion = self._get_previous_conversion_rate()
  +        
  +        if previous_conversion > 0:
  +            drop_percentage = (previous_conversion - current_conversion) / previous_conversion
  +            
  +            if drop_percentage > 0.15:  # More than 15% drop
  +                alerts.append(Alert(
  +                    alert_id=f"conversion_drop_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
  +                    alert_type='conversion_drop',
  +                    severity='high',
  +                    title="Drop in Conversion Rate",
  +                    description=f"Conversion rate dropped by {drop_percentage:.1%}",
  +                    data={
  +                        'current_conversion': current_conversion,
  +                        'previous_conversion': previous_conversion,
  +                        'drop_percentage': drop_percentage
  +                    },
  +                    detected_at=datetime.now()
  +                ))
  +        
  +        # Check revenue anomalies
  +        current_revenue = self._get_current_revenue()
  +        previous_revenue = self._get_previous_revenue()
  +        
  +        if previous_revenue > 0:
  +            drop_percentage = (previous_revenue - current_revenue) / previous_revenue
  +            
  +            if drop_percentage > 0.25:  # More than 25% drop
  +                alerts.append(Alert(
  +                    alert_id=f"revenue_drop_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
  +                    alert_type='conversion_drop',
  +                    severity='critical',
  +                    title="Significant Revenue Drop",
  +                    description=f"Revenue dropped by {drop_percentage:.1%}",
  +                    data={
  +                        'current_revenue': current_revenue,
  +                        'previous_revenue': previous_revenue,
  +                        'drop_percentage': drop_percentage
  +                    },
  +                    detected_at=datetime.now()
  +                ))
  +        
  +        return alerts
  +    
  +    def _check_technical_anomalies(self) -> List[Alert]:
  +        """Check for technical anomalies"""
  +        alerts = []
  +        
  +        # Check error rate
  +        error_count = self._get_recent_error_count()
  +        
  +        if error_count > 10:  # More than 10 errors in last hour
  +            alerts.append(Alert(
  +                alert_id=f"technical_errors_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
  +                alert_type='technical_errors',
  +                severity='critical',
  +                title="High Error Rate Detected",
  +                description=f"{error_count} errors detected in the last hour",
  +                data={'error_count': error_count},
  +                detected_at=datetime.now()
  +            ))
  +        
  +        # Check system performance
  +        avg_response_time = self._get_avg_response_time()
  +        
  +        if avg_response_time > 5000:  # More than 5 seconds
  +            alerts.append(Alert(
  +                alert_id=f"performance_issue_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
  +                alert_type='technical_errors',
  +                severity='medium',
  +                title="Performance Degradation",
  +                description=f"Average response time is {avg_response_time}ms",
  +                data={'avg_response_time': avg_response_time},
  +                detected_at=datetime.now()
  +            ))
  +        
  +        return alerts
  +    
  +    def _check_content_anomalies(self) -> List[Alert]:
  +        """Check for content-related anomalies"""
  +        alerts = []
  +        
  +        # Check for content with unusually low engagement
  +        low_engagement_content = self._get_low_engagement_content()
  +        
  +        if low_engagement_content:
  +            alerts.append(Alert(
  +                alert_id=f"content_engagement_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
  +                alert_type='content_anomaly',
  +                severity='low',
  +                title="Content with Low Engagement",
  +                description=f"{len(low_engagement_content)} content items have below-average engagement",
  +                data={'low_engagement_content': low_engagement_content},
  +                detected_at=datetime.now()
  +            ))
  +        
  +        return alerts
  +    
  +    def _check_user_anomalies(self) -> List[Alert]:
  +        """Check for user behavior anomalies"""
  +        alerts = []
  +        
  +        # Check for high-value users at risk
  +        at_risk_users = self._get_high_value_users_at_risk()
  +        
  +        if at_risk_users:
  +            alerts.append(Alert(
  +                alert_id=f"high_value_risk_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
  +                alert_type='high_value_user_risk',
  +                severity='medium',
  +                title="High-Value Users at Risk",
  +                description=f"{len(at_risk_users)} high-value users showing signs of churn",
  +                data={'at_risk_users': at_risk_users},
  +                detected_at=datetime.now()
  +            ))
  +        
  +        return alerts
  +    
  +    def _notify_admin(self, alert: Alert) -> bool:
  +        """Internal method to send admin notifications"""
  +        return self.notify_admin(alert)
  ```
  ----------------------------------------------------------
  üìç L√≠nea 370 | @gemini-code-assist[bot]:
  üí¨ ![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)

The import `from database.models import AnalyticsEvent` is done locally inside the method. This pattern is repeated in several methods in this file. It's generally better to place all imports at the top of the file for clarity, to avoid repeated imports, and to make dependencies explicit. If this is done to avoid circular dependencies, it might indicate a need to refactor the project structure.
  
  C√≥digo:
  ```
  @@ -0,0 +1,465 @@
  +"""
  +Alert System for DianaBot Analytics System
  +Detects anomalies and sends notifications to admins
  +"""
  +
  +import logging
  +import asyncio
  +from datetime import datetime, timedelta
  +from typing import Dict, List, Optional, Any, Tuple
  +from dataclasses import dataclass
  +from sqlalchemy import func, and_, or_, text
  +from sqlalchemy.orm import Session
  +
  +logger = logging.getLogger(__name__)
  +
  +
  +@dataclass
  +class Alert:
  +    """Alert data structure"""
  +    alert_id: str
  +    alert_type: str
  +    severity: str  # low, medium, high, critical
  +    title: str
  +    description: str
  +    data: Dict[str, Any]
  +    detected_at: datetime
  +    acknowledged: bool = False
  +    acknowledged_by: Optional[str] = None
  +    acknowledged_at: Optional[datetime] = None
  +
  +
  +@dataclass
  +class AlertConfig:
  +    """Alert configuration"""
  +    alert_type: str
  +    enabled: bool
  +    threshold: float
  +    comparison: str  # "greater_than", "less_than", "equal"
  +    time_window_hours: int
  +    notification_channels: List[str]  # "telegram", "email", "dashboard"
  +
  +
  +class AlertSystem:
  +    """Detects anomalies and sends notifications to admins"""
  +    
  +    def __init__(self, db_session: Session):
  +        self.db = db_session
  +        self.alert_configs = self._load_alert_configs()
  +    
  +    def check_anomalies(self) -> List[Alert]:
  +        """Check for all types of anomalies and return alerts"""
  +        logger.info("Checking for anomalies across all systems")
  +        
  +        alerts = []
  +        
  +        # Check engagement anomalies
  +        engagement_alerts = self._check_engagement_anomalies()
  +        alerts.extend(engagement_alerts)
  +        
  +        # Check monetization anomalies
  +        monetization_alerts = self._check_monetization_anomalies()
  +        alerts.extend(monetization_alerts)
  +        
  +        # Check technical anomalies
  +        technical_alerts = self._check_technical_anomalies()
  +        alerts.extend(technical_alerts)
  +        
  +        # Check content anomalies
  +        content_alerts = self._check_content_anomalies()
  +        alerts.extend(content_alerts)
  +        
  +        # Check user behavior anomalies
  +        user_alerts = self._check_user_anomalies()
  +        alerts.extend(user_alerts)
  +        
  +        # Send notifications for critical alerts
  +        for alert in alerts:
  +            if alert.severity in ['high', 'critical']:
  +                self._notify_admin(alert)
  +        
  +        return alerts
  +    
  +    def notify_admin(self, alert: Alert) -> bool:
  +        """Send notification to admin via configured channels"""
  +        logger.info(f"Sending admin notification for alert: {alert.alert_type}")
  +        
  +        config = self.alert_configs.get(alert.alert_type)
  +        if not config or not config.enabled:
  +            logger.warning(f"Alert type {alert.alert_type} is disabled or not configured")
  +            return False
  +        
  +        success = True
  +        
  +        # Send via Telegram
  +        if 'telegram' in config.notification_channels:
  +            success = success and self._send_telegram_notification(alert)
  +        
  +        # Send via Email
  +        if 'email' in config.notification_channels:
  +            success = success and self._send_email_notification(alert)
  +        
  +        # Send to Dashboard
  +        if 'dashboard' in config.notification_channels:
  +            success = success and self._send_dashboard_notification(alert)
  +        
  +        return success
  +    
  +    def get_alert_config(self, alert_type: str) -> Optional[AlertConfig]:
  +        """Get configuration for specific alert type"""
  +        return self.alert_configs.get(alert_type)
  +    
  +    def update_alert_config(self, alert_type: str, config: AlertConfig) -> bool:
  +        """Update configuration for alert type"""
  +        self.alert_configs[alert_type] = config
  +        return True
  +    
  +    def acknowledge_alert(self, alert_id: str, acknowledged_by: str) -> bool:
  +        """Mark alert as acknowledged"""
  +        # In production, this would update the alert in database
  +        logger.info(f"Alert {alert_id} acknowledged by {acknowledged_by}")
  +        return True
  +    
  +    # Private helper methods
  +    
  +    def _load_alert_configs(self) -> Dict[str, AlertConfig]:
  +        """Load alert configurations"""
  +        return {
  +            'engagement_drop': AlertConfig(
  +                alert_type='engagement_drop',
  +                enabled=True,
  +                threshold=0.2,  # 20% drop
  +                comparison='less_than',
  +                time_window_hours=24,
  +                notification_channels=['telegram', 'dashboard']
  +            ),
  +            'conversion_drop': AlertConfig(
  +                alert_type='conversion_drop',
  +                enabled=True,
  +                threshold=0.15,  # 15% drop
  +                comparison='less_than',
  +                time_window_hours=24,
  +                notification_channels=['telegram', 'email', 'dashboard']
  +            ),
  +            'technical_errors': AlertConfig(
  +                alert_type='technical_errors',
  +                enabled=True,
  +                threshold=10,  # 10 errors
  +                comparison='greater_than',
  +                time_window_hours=1,
  +                notification_channels=['telegram', 'email']
  +            ),
  +            'high_value_user_risk': AlertConfig(
  +                alert_type='high_value_user_risk',
  +                enabled=True,
  +                threshold=0.7,  # 70% probability
  +                comparison='greater_than',
  +                time_window_hours=24,
  +                notification_channels=['dashboard']
  +            ),
  +            'besitos_economy_anomaly': AlertConfig(
  +                alert_type='besitos_economy_anomaly',
  +                enabled=True,
  +                threshold=0.3,  # 30% deviation
  +                comparison='greater_than',
  +                time_window_hours=24,
  +                notification_channels=['dashboard']
  +            )
  +        }
  +    
  +    def _check_engagement_anomalies(self) -> List[Alert]:
  +        """Check for engagement anomalies"""
  +        alerts = []
  +        
  +        # Check DAU drop
  +        current_dau = self._get_current_dau()
  +        previous_dau = self._get_previous_dau()
  +        
  +        if previous_dau > 0:
  +            drop_percentage = (previous_dau - current_dau) / previous_dau
  +            
  +            if drop_percentage > 0.2:  # More than 20% drop
  +                alerts.append(Alert(
  +                    alert_id=f"engagement_drop_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
  +                    alert_type='engagement_drop',
  +                    severity='high',
  +                    title="Significant Drop in Daily Active Users",
  +                    description=f"DAU dropped by {drop_percentage:.1%} compared to previous period",
  +                    data={
  +                        'current_dau': current_dau,
  +                        'previous_dau': previous_dau,
  +                        'drop_percentage': drop_percentage
  +                    },
  +                    detected_at=datetime.now()
  +                ))
  +        
  +        # Check session duration drop
  +        current_session_duration = self._get_current_session_duration()
  +        previous_session_duration = self._get_previous_session_duration()
  +        
  +        if previous_session_duration > 0:
  +            drop_percentage = (previous_session_duration - current_session_duration) / previous_session_duration
  +            
  +            if drop_percentage > 0.3:  # More than 30% drop
  +                alerts.append(Alert(
  +                    alert_id=f"session_duration_drop_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
  +                    alert_type='engagement_drop',
  +                    severity='medium',
  +                    title="Drop in Average Session Duration",
  +                    description=f"Average session duration dropped by {drop_percentage:.1%}",
  +                    data={
  +                        'current_duration': current_session_duration,
  +                        'previous_duration': previous_session_duration,
  +                        'drop_percentage': drop_percentage
  +                    },
  +                    detected_at=datetime.now()
  +                ))
  +        
  +        return alerts
  +    
  +    def _check_monetization_anomalies(self) -> List[Alert]:
  +        """Check for monetization anomalies"""
  +        alerts = []
  +        
  +        # Check conversion rate drop
  +        current_conversion = self._get_current_conversion_rate()
  +        previous_conversion = self._get_previous_conversion_rate()
  +        
  +        if previous_conversion > 0:
  +            drop_percentage = (previous_conversion - current_conversion) / previous_conversion
  +            
  +            if drop_percentage > 0.15:  # More than 15% drop
  +                alerts.append(Alert(
  +                    alert_id=f"conversion_drop_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
  +                    alert_type='conversion_drop',
  +                    severity='high',
  +                    title="Drop in Conversion Rate",
  +                    description=f"Conversion rate dropped by {drop_percentage:.1%}",
  +                    data={
  +                        'current_conversion': current_conversion,
  +                        'previous_conversion': previous_conversion,
  +                        'drop_percentage': drop_percentage
  +                    },
  +                    detected_at=datetime.now()
  +                ))
  +        
  +        # Check revenue anomalies
  +        current_revenue = self._get_current_revenue()
  +        previous_revenue = self._get_previous_revenue()
  +        
  +        if previous_revenue > 0:
  +            drop_percentage = (previous_revenue - current_revenue) / previous_revenue
  +            
  +            if drop_percentage > 0.25:  # More than 25% drop
  +                alerts.append(Alert(
  +                    alert_id=f"revenue_drop_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
  +                    alert_type='conversion_drop',
  +                    severity='critical',
  +                    title="Significant Revenue Drop",
  +                    description=f"Revenue dropped by {drop_percentage:.1%}",
  +                    data={
  +                        'current_revenue': current_revenue,
  +                        'previous_revenue': previous_revenue,
  +                        'drop_percentage': drop_percentage
  +                    },
  +                    detected_at=datetime.now()
  +                ))
  +        
  +        return alerts
  +    
  +    def _check_technical_anomalies(self) -> List[Alert]:
  +        """Check for technical anomalies"""
  +        alerts = []
  +        
  +        # Check error rate
  +        error_count = self._get_recent_error_count()
  +        
  +        if error_count > 10:  # More than 10 errors in last hour
  +            alerts.append(Alert(
  +                alert_id=f"technical_errors_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
  +                alert_type='technical_errors',
  +                severity='critical',
  +                title="High Error Rate Detected",
  +                description=f"{error_count} errors detected in the last hour",
  +                data={'error_count': error_count},
  +                detected_at=datetime.now()
  +            ))
  +        
  +        # Check system performance
  +        avg_response_time = self._get_avg_response_time()
  +        
  +        if avg_response_time > 5000:  # More than 5 seconds
  +            alerts.append(Alert(
  +                alert_id=f"performance_issue_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
  +                alert_type='technical_errors',
  +                severity='medium',
  +                title="Performance Degradation",
  +                description=f"Average response time is {avg_response_time}ms",
  +                data={'avg_response_time': avg_response_time},
  +                detected_at=datetime.now()
  +            ))
  +        
  +        return alerts
  +    
  +    def _check_content_anomalies(self) -> List[Alert]:
  +        """Check for content-related anomalies"""
  +        alerts = []
  +        
  +        # Check for content with unusually low engagement
  +        low_engagement_content = self._get_low_engagement_content()
  +        
  +        if low_engagement_content:
  +            alerts.append(Alert(
  +                alert_id=f"content_engagement_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
  +                alert_type='content_anomaly',
  +                severity='low',
  +                title="Content with Low Engagement",
  +                description=f"{len(low_engagement_content)} content items have below-average engagement",
  +                data={'low_engagement_content': low_engagement_content},
  +                detected_at=datetime.now()
  +            ))
  +        
  +        return alerts
  +    
  +    def _check_user_anomalies(self) -> List[Alert]:
  +        """Check for user behavior anomalies"""
  +        alerts = []
  +        
  +        # Check for high-value users at risk
  +        at_risk_users = self._get_high_value_users_at_risk()
  +        
  +        if at_risk_users:
  +            alerts.append(Alert(
  +                alert_id=f"high_value_risk_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
  +                alert_type='high_value_user_risk',
  +                severity='medium',
  +                title="High-Value Users at Risk",
  +                description=f"{len(at_risk_users)} high-value users showing signs of churn",
  +                data={'at_risk_users': at_risk_users},
  +                detected_at=datetime.now()
  +            ))
  +        
  +        return alerts
  +    
  +    def _notify_admin(self, alert: Alert) -> bool:
  +        """Internal method to send admin notifications"""
  +        return self.notify_admin(alert)
  +    
  +    def _send_telegram_notification(self, alert: Alert) -> bool:
  +        """Send notification via Telegram"""
  +        # In production, this would integrate with Telegram Bot API
  +        logger.info(f"[TELEGRAM] {alert.severity.upper()}: {alert.title} - {alert.description}")
  +        return True
  +    
  +    def _send_email_notification(self, alert: Alert) -> bool:
  +        """Send notification via Email"""
  +        # In production, this would integrate with email service
  +        logger.info(f"[EMAIL] {alert.severity.upper()}: {alert.title} - {alert.description}")
  +        return True
  +    
  +    def _send_dashboard_notification(self, alert: Alert) -> bool:
  +        """Send notification to dashboard"""
  +        # In production, this would update dashboard state
  +        logger.info(f"[DASHBOARD] {alert.severity.upper()}: {alert.title} - {alert.description}")
  +        return True
  +    
  +    # Data access methods (simplified implementations)
  +    
  +    def _get_current_dau(self) -> int:
  +        """Get current daily active users"""
  +        from database.models import AnalyticsEvent
  ```
  ----------------------------------------------------------

üìÑ ARCHIVO: modules/analytics/export.py
============================================================
  üìç L√≠nea 153 | @gemini-code-assist[bot]:
  üí¨ ![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)

The import `from database.models import ...` is done locally inside the method. This pattern is repeated in several methods in this file. It's generally better to place all imports at the top of the file for clarity, to avoid repeated imports, and to make dependencies explicit. If this is done to avoid circular dependencies, it might indicate a need to refactor the project structure.
  
  C√≥digo:
  ```
  @@ -0,0 +1,599 @@
  +"""
  +Data Export System for DianaBot Analytics
  +Exports analytics data in multiple formats
  +"""
  +
  +import logging
  +import json
  +import csv
  +import io
  +from datetime import datetime, timedelta
  +from typing import Dict, List, Optional, Any, Tuple
  +from dataclasses import dataclass
  +from sqlalchemy.orm import Session
  +
  +logger = logging.getLogger(__name__)
  +
  +
  +@dataclass
  +class ExportRequest:
  +    """Export request data structure"""
  +    export_id: str
  +    export_type: str  # "user_data", "analytics", "transactions", "content"
  +    format: str  # "json", "csv", "excel"
  +    filters: Dict[str, Any]
  +    time_range_start: Optional[datetime] = None
  +    time_range_end: Optional[datetime] = None
  +    include_sensitive: bool = False
  +
  +
  +@dataclass
  +class ExportResult:
  +    """Export result data structure"""
  +    export_id: str
  +    export_type: str
  +    format: str
  +    generated_at: datetime
  +    file_size: int
  +    download_url: Optional[str] = None
  +    content: Optional[str] = None
  +    status: str = "completed"  # "completed", "failed", "processing"
  +
  +
  +class DataExporter:
  +    """Exports analytics data in multiple formats"""
  +    
  +    def __init__(self, db_session: Session):
  +        self.db = db_session
  +    
  +    def export_data(self, request: ExportRequest) -> ExportResult:
  +        """Export data based on request"""
  +        logger.info(f"Exporting data: {request.export_type} in {request.format}")
  +        
  +        try:
  +            # Generate data based on export type
  +            data = self._generate_export_data(request)
  +            
  +            # Convert to requested format
  +            content = self._convert_to_format(data, request.format, request)
  +            
  +            # Calculate file size
  +            file_size = len(content.encode('utf-8')) if content else 0
  +            
  +            return ExportResult(
  +                export_id=request.export_id,
  +                export_type=request.export_type,
  +                format=request.format,
  +                generated_at=datetime.now(),
  +                file_size=file_size,
  +                content=content,
  +                status="completed"
  +            )
  +            
  +        except Exception as e:
  +            logger.error(f"Export failed: {e}")
  +            return ExportResult(
  +                export_id=request.export_id,
  +                export_type=request.export_type,
  +                format=request.format,
  +                generated_at=datetime.now(),
  +                file_size=0,
  +                status="failed"
  +            )
  +    
  +    def export_user_data(self, user_id: int, format: str = "json", 
  +                        include_sensitive: bool = False) -> ExportResult:
  +        """Export user data for GDPR compliance"""
  +        request = ExportRequest(
  +            export_id=f"user_{user_id}_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
  +            export_type="user_data",
  +            format=format,
  +            filters={'user_id': user_id},
  +            include_sensitive=include_sensitive
  +        )
  +        
  +        return self.export_data(request)
  +    
  +    def export_analytics_data(self, time_range_start: datetime, 
  +                            time_range_end: datetime, format: str = "csv") -> ExportResult:
  +        """Export analytics data for external analysis"""
  +        request = ExportRequest(
  +            export_id=f"analytics_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
  +            export_type="analytics",
  +            format=format,
  +            filters={},
  +            time_range_start=time_range_start,
  +            time_range_end=time_range_end
  +        )
  +        
  +        return self.export_data(request)
  +    
  +    def export_transaction_data(self, time_range_start: datetime, 
  +                              time_range_end: datetime, format: str = "excel") -> ExportResult:
  +        """Export transaction data for accounting"""
  +        request = ExportRequest(
  +            export_id=f"transactions_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
  +            export_type="transactions",
  +            format=format,
  +            filters={},
  +            time_range_start=time_range_start,
  +            time_range_end=time_range_end
  +        )
  +        
  +        return self.export_data(request)
  +    
  +    def export_content_data(self, format: str = "json") -> ExportResult:
  +        """Export content data for backup or migration"""
  +        request = ExportRequest(
  +            export_id=f"content_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
  +            export_type="content",
  +            format=format,
  +            filters={}
  +        )
  +        
  +        return self.export_data(request)
  +    
  +    # Private helper methods
  +    
  +    def _generate_export_data(self, request: ExportRequest) -> Dict[str, Any]:
  +        """Generate data for export based on request type"""
  +        if request.export_type == "user_data":
  +            return self._generate_user_data(request)
  +        elif request.export_type == "analytics":
  +            return self._generate_analytics_data(request)
  +        elif request.export_type == "transactions":
  +            return self._generate_transaction_data(request)
  +        elif request.export_type == "content":
  +            return self._generate_content_data(request)
  +        else:
  +            raise ValueError(f"Unsupported export type: {request.export_type}")
  +    
  +    def _generate_user_data(self, request: ExportRequest) -> Dict[str, Any]:
  +        """Generate user data for export"""
  +        from database.models import User, Subscription, UserBalance, UserInventory
  ```
  ----------------------------------------------------------
  üìç L√≠nea 495 | @gemini-code-assist[bot]:
  üí¨ ![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)

Embedding a JSON string within a CSV cell (`json.dumps(item)`) can make the resulting CSV file difficult to parse and analyze with standard tools. It would be better to flatten the nested dictionary into separate columns. For example, if `item` is `{'key1': 'val1', 'key2': 'val2'}`, you could have columns `key1` and `key2`.
  
  C√≥digo:
  ```
  @@ -0,0 +1,599 @@
  +"""
  +Data Export System for DianaBot Analytics
  +Exports analytics data in multiple formats
  +"""
  +
  +import logging
  +import json
  +import csv
  +import io
  +from datetime import datetime, timedelta
  +from typing import Dict, List, Optional, Any, Tuple
  +from dataclasses import dataclass
  +from sqlalchemy.orm import Session
  +
  +logger = logging.getLogger(__name__)
  +
  +
  +@dataclass
  +class ExportRequest:
  +    """Export request data structure"""
  +    export_id: str
  +    export_type: str  # "user_data", "analytics", "transactions", "content"
  +    format: str  # "json", "csv", "excel"
  +    filters: Dict[str, Any]
  +    time_range_start: Optional[datetime] = None
  +    time_range_end: Optional[datetime] = None
  +    include_sensitive: bool = False
  +
  +
  +@dataclass
  +class ExportResult:
  +    """Export result data structure"""
  +    export_id: str
  +    export_type: str
  +    format: str
  +    generated_at: datetime
  +    file_size: int
  +    download_url: Optional[str] = None
  +    content: Optional[str] = None
  +    status: str = "completed"  # "completed", "failed", "processing"
  +
  +
  +class DataExporter:
  +    """Exports analytics data in multiple formats"""
  +    
  +    def __init__(self, db_session: Session):
  +        self.db = db_session
  +    
  +    def export_data(self, request: ExportRequest) -> ExportResult:
  +        """Export data based on request"""
  +        logger.info(f"Exporting data: {request.export_type} in {request.format}")
  +        
  +        try:
  +            # Generate data based on export type
  +            data = self._generate_export_data(request)
  +            
  +            # Convert to requested format
  +            content = self._convert_to_format(data, request.format, request)
  +            
  +            # Calculate file size
  +            file_size = len(content.encode('utf-8')) if content else 0
  +            
  +            return ExportResult(
  +                export_id=request.export_id,
  +                export_type=request.export_type,
  +                format=request.format,
  +                generated_at=datetime.now(),
  +                file_size=file_size,
  +                content=content,
  +                status="completed"
  +            )
  +            
  +        except Exception as e:
  +            logger.error(f"Export failed: {e}")
  +            return ExportResult(
  +                export_id=request.export_id,
  +                export_type=request.export_type,
  +                format=request.format,
  +                generated_at=datetime.now(),
  +                file_size=0,
  +                status="failed"
  +            )
  +    
  +    def export_user_data(self, user_id: int, format: str = "json", 
  +                        include_sensitive: bool = False) -> ExportResult:
  +        """Export user data for GDPR compliance"""
  +        request = ExportRequest(
  +            export_id=f"user_{user_id}_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
  +            export_type="user_data",
  +            format=format,
  +            filters={'user_id': user_id},
  +            include_sensitive=include_sensitive
  +        )
  +        
  +        return self.export_data(request)
  +    
  +    def export_analytics_data(self, time_range_start: datetime, 
  +                            time_range_end: datetime, format: str = "csv") -> ExportResult:
  +        """Export analytics data for external analysis"""
  +        request = ExportRequest(
  +            export_id=f"analytics_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
  +            export_type="analytics",
  +            format=format,
  +            filters={},
  +            time_range_start=time_range_start,
  +            time_range_end=time_range_end
  +        )
  +        
  +        return self.export_data(request)
  +    
  +    def export_transaction_data(self, time_range_start: datetime, 
  +                              time_range_end: datetime, format: str = "excel") -> ExportResult:
  +        """Export transaction data for accounting"""
  +        request = ExportRequest(
  +            export_id=f"transactions_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
  +            export_type="transactions",
  +            format=format,
  +            filters={},
  +            time_range_start=time_range_start,
  +            time_range_end=time_range_end
  +        )
  +        
  +        return self.export_data(request)
  +    
  +    def export_content_data(self, format: str = "json") -> ExportResult:
  +        """Export content data for backup or migration"""
  +        request = ExportRequest(
  +            export_id=f"content_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
  +            export_type="content",
  +            format=format,
  +            filters={}
  +        )
  +        
  +        return self.export_data(request)
  +    
  +    # Private helper methods
  +    
  +    def _generate_export_data(self, request: ExportRequest) -> Dict[str, Any]:
  +        """Generate data for export based on request type"""
  +        if request.export_type == "user_data":
  +            return self._generate_user_data(request)
  +        elif request.export_type == "analytics":
  +            return self._generate_analytics_data(request)
  +        elif request.export_type == "transactions":
  +            return self._generate_transaction_data(request)
  +        elif request.export_type == "content":
  +            return self._generate_content_data(request)
  +        else:
  +            raise ValueError(f"Unsupported export type: {request.export_type}")
  +    
  +    def _generate_user_data(self, request: ExportRequest) -> Dict[str, Any]:
  +        """Generate user data for export"""
  +        from database.models import User, Subscription, UserBalance, UserInventory
  +        
  +        user_id = request.filters.get('user_id')
  +        if not user_id:
  +            raise ValueError("User ID is required for user data export")
  +        
  +        # Get user data
  +        user = self.db.query(User).filter(User.id == user_id).first()
  +        if not user:
  +            raise ValueError(f"User not found: {user_id}")
  +        
  +        user_data = {
  +            'user_info': {
  +                'id': user.id,
  +                'telegram_id': user.telegram_id,
  +                'username': user.username,
  +                'first_name': user.first_name,
  +                'last_name': user.last_name,
  +                'created_at': user.created_at.isoformat() if user.created_at else None,
  +                'last_login': user.last_login.isoformat() if user.last_login else None
  +            },
  +            'subscriptions': [],
  +            'balance': {},
  +            'inventory': []
  +        }
  +        
  +        # Get subscription data
  +        subscriptions = self.db.query(Subscription).filter(Subscription.user_id == user_id).all()
  +        for sub in subscriptions:
  +            user_data['subscriptions'].append({
  +                'id': sub.id,
  +                'status': sub.status,
  +                'start_date': sub.start_date.isoformat() if sub.start_date else None,
  +                'end_date': sub.end_date.isoformat() if sub.end_date else None,
  +                'created_at': sub.created_at.isoformat() if sub.created_at else None
  +            })
  +        
  +        # Get balance data
  +        balance = self.db.query(UserBalance).filter(UserBalance.user_id == user_id).first()
  +        if balance:
  +            user_data['balance'] = {
  +                'besitos': balance.besitos,
  +                'premium_currency': balance.premium_currency,
  +                'updated_at': balance.updated_at.isoformat() if balance.updated_at else None
  +            }
  +        
  +        # Get inventory data
  +        inventory = self.db.query(UserInventory).filter(UserInventory.user_id == user_id).all()
  +        for item in inventory:
  +            user_data['inventory'].append({
  +                'item_id': item.item_id,
  +                'quantity': item.quantity,
  +                'acquired_at': item.acquired_at.isoformat() if item.acquired_at else None
  +            })
  +        
  +        # Include sensitive data if requested
  +        if request.include_sensitive:
  +            # Add any sensitive data here
  +            pass
  +        
  +        return user_data
  +    
  +    def _generate_analytics_data(self, request: ExportRequest) -> Dict[str, Any]:
  +        """Generate analytics data for export"""
  +        from database.models import AnalyticsEvent
  +        from .aggregator import MetricsAggregator, TimeRange
  +        
  +        time_range = TimeRange(
  +            request.time_range_start or (datetime.now() - timedelta(days=30)),
  +            request.time_range_end or datetime.now()
  +        )
  +        
  +        aggregator = MetricsAggregator(self.db)
  +        
  +        # Get aggregated metrics
  +        engagement_metrics = aggregator.get_engagement_metrics(time_range)
  +        monetization_metrics = aggregator.get_monetization_metrics(time_range)
  +        narrative_metrics = aggregator.get_narrative_metrics(time_range)
  +        experience_metrics = aggregator.get_experience_metrics(time_range)
  +        
  +        # Get raw events (limited for performance)
  +        events = self.db.query(AnalyticsEvent).filter(
  +            AnalyticsEvent.timestamp >= time_range.start_date,
  +            AnalyticsEvent.timestamp <= time_range.end_date
  +        ).limit(1000).all()
  +        
  +        analytics_data = {
  +            'time_range': {
  +                'start': time_range.start_date.isoformat(),
  +                'end': time_range.end_date.isoformat()
  +            },
  +            'aggregated_metrics': {
  +                'engagement': {
  +                    'mau': engagement_metrics.mau,
  +                    'dau': engagement_metrics.dau,
  +                    'retention_d1': engagement_metrics.retention_d1,
  +                    'retention_d7': engagement_metrics.retention_d7,
  +                    'retention_d30': engagement_metrics.retention_d30,
  +                    'avg_session_duration': engagement_metrics.avg_session_duration,
  +                    'engagement_by_module': engagement_metrics.engagement_by_module
  +                },
  +                'monetization': {
  +                    'total_revenue': monetization_metrics.total_revenue,
  +                    'arpu': monetization_metrics.arpu,
  +                    'arppu': monetization_metrics.arppu,
  +                    'free_to_vip_conversion': monetization_metrics.free_to_vip_conversion,
  +                    'ltv': monetization_metrics.ltv,
  +                    'revenue_by_product': monetization_metrics.revenue_by_product
  +                },
  +                'narrative': {
  +                    'most_visited_fragments': narrative_metrics.most_visited_fragments,
  +                    'completion_rate': narrative_metrics.completion_rate,
  +                    'popular_decisions': narrative_metrics.popular_decisions,
  +                    'drop_off_points': narrative_metrics.drop_off_points
  +                },
  +                'experiences': {
  +                    'start_rate': experience_metrics.start_rate,
  +                    'completion_rate': experience_metrics.completion_rate,
  +                    'avg_completion_time': experience_metrics.avg_completion_time,
  +                    'popular_experiences': experience_metrics.popular_experiences
  +                }
  +            },
  +            'sample_events': [
  +                {
  +                    'id': event.id,
  +                    'user_id': event.user_id,
  +                    'event_type': event.event_type,
  +                    'timestamp': event.timestamp.isoformat(),
  +                    'metadata': event.metadata
  +                }
  +                for event in events
  +            ]
  +        }
  +        
  +        return analytics_data
  +    
  +    def _generate_transaction_data(self, request: ExportRequest) -> Dict[str, Any]:
  +        """Generate transaction data for export"""
  +        from database.models import Transaction
  +        
  +        time_range_start = request.time_range_start or (datetime.now() - timedelta(days=30))
  +        time_range_end = request.time_range_end or datetime.now()
  +        
  +        transactions = self.db.query(Transaction).filter(
  +            Transaction.created_at >= time_range_start,
  +            Transaction.created_at <= time_range_end
  +        ).all()
  +        
  +        transaction_data = {
  +            'time_range': {
  +                'start': time_range_start.isoformat(),
  +                'end': time_range_end.isoformat()
  +            },
  +            'transactions': [
  +                {
  +                    'id': tx.id,
  +                    'user_id': tx.user_id,
  +                    'amount': float(tx.amount) if tx.amount else 0,
  +                    'currency': tx.currency,
  +                    'product_type': tx.product_type,
  +                    'status': tx.status,
  +                    'created_at': tx.created_at.isoformat() if tx.created_at else None,
  +                    'completed_at': tx.completed_at.isoformat() if tx.completed_at else None
  +                }
  +                for tx in transactions
  +            ],
  +            'summary': {
  +                'total_transactions': len(transactions),
  +                'total_revenue': sum(float(tx.amount) for tx in transactions if tx.amount),
  +                'completed_transactions': len([tx for tx in transactions if tx.status == 'completed']),
  +                'failed_transactions': len([tx for tx in transactions if tx.status == 'failed'])
  +            }
  +        }
  +        
  +        return transaction_data
  +    
  +    def _generate_content_data(self, request: ExportRequest) -> Dict[str, Any]:
  +        """Generate content data for export"""
  +        from database.models import NarrativeFragment, Mission, Achievement, ShopItem
  +        
  +        content_data = {
  +            'narrative_fragments': [],
  +            'missions': [],
  +            'achievements': [],
  +            'shop_items': []
  +        }
  +        
  +        # Get narrative fragments
  +        fragments = self.db.query(NarrativeFragment).filter(NarrativeFragment.is_active == True).all()
  +        for fragment in fragments:
  +            content_data['narrative_fragments'].append({
  +                'id': fragment.id,
  +                'fragment_key': fragment.fragment_key,
  +                'title': fragment.title,
  +                'content': fragment.content,
  +                'is_active': fragment.is_active,
  +                'created_at': fragment.created_at.isoformat() if fragment.created_at else None
  +            })
  +        
  +        # Get missions
  +        missions = self.db.query(Mission).filter(Mission.is_active == True).all()
  +        for mission in missions:
  +            content_data['missions'].append({
  +                'id': mission.id,
  +                'mission_key': mission.mission_key,
  +                'title': mission.title,
  +                'description': mission.description,
  +                'requirements': mission.requirements,
  +                'rewards': mission.rewards,
  +                'is_active': mission.is_active,
  +                'created_at': mission.created_at.isoformat() if mission.created_at else None
  +            })
  +        
  +        # Get achievements
  +        achievements = self.db.query(Achievement).all()
  +        for achievement in achievements:
  +            content_data['achievements'].append({
  +                'id': achievement.id,
  +                'achievement_key': achievement.achievement_key,
  +                'name': achievement.name,
  +                'description': achievement.description,
  +                'requirements': achievement.requirements,
  +                'rewards': achievement.rewards,
  +                'created_at': achievement.created_at.isoformat() if achievement.created_at else None
  +            })
  +        
  +        # Get shop items
  +        shop_items = self.db.query(ShopItem).all()
  +        for item in shop_items:
  +            content_data['shop_items'].append({
  +                'id': item.id,
  +                'item_key': item.item_key,
  +                'name': item.name,
  +                'description': item.description,
  +                'item_type': item.item_type,
  +                'rarity': item.rarity,
  +                'price_besitos': item.price_besitos,
  +                'price_premium': item.price_premium,
  +                'created_at': item.created_at.isoformat() if item.created_at else None
  +            })
  +        
  +        return content_data
  +    
  +    def _convert_to_format(self, data: Dict[str, Any], format: str, 
  +                          request: ExportRequest) -> str:
  +        """Convert data to requested format"""
  +        if format == "json":
  +            return json.dumps(data, indent=2, ensure_ascii=False, default=str)
  +        
  +        elif format == "csv":
  +            return self._convert_to_csv(data, request)
  +        
  +        elif format == "excel":
  +            # In production, this would use openpyxl or similar
  +            # For now, return CSV as placeholder
  +            return self._convert_to_csv(data, request)
  +        
  +        else:
  +            raise ValueError(f"Unsupported export format: {format}")
  +    
  +    def _convert_to_csv(self, data: Dict[str, Any], request: ExportRequest) -> str:
  +        """Convert data to CSV format"""
  +        output = io.StringIO()
  +        writer = csv.writer(output)
  +        
  +        if request.export_type == "user_data":
  +            self._write_user_data_csv(writer, data)
  +        elif request.export_type == "analytics":
  +            self._write_analytics_data_csv(writer, data)
  +        elif request.export_type == "transactions":
  +            self._write_transaction_data_csv(writer, data)
  +        elif request.export_type == "content":
  +            self._write_content_data_csv(writer, data)
  +        
  +        return output.getvalue()
  +    
  +    def _write_user_data_csv(self, writer: csv.writer, data: Dict[str, Any]):
  +        """Write user data to CSV"""
  +        writer.writerow(['USER DATA EXPORT'])
  +        writer.writerow(['Generated At', datetime.now().isoformat()])
  +        writer.writerow([])
  +        
  +        # User info
  +        writer.writerow(['USER INFORMATION'])
  +        user_info = data.get('user_info', {})
  +        for key, value in user_info.items():
  +            writer.writerow([key, value])
  +        writer.writerow([])
  +        
  +        # Subscriptions
  +        writer.writerow(['SUBSCRIPTIONS'])
  +        writer.writerow(['ID', 'Status', 'Start Date', 'End Date', 'Created At'])
  +        for sub in data.get('subscriptions', []):
  +            writer.writerow([
  +                sub.get('id'),
  +                sub.get('status'),
  +                sub.get('start_date'),
  +                sub.get('end_date'),
  +                sub.get('created_at')
  +            ])
  +        writer.writerow([])
  +        
  +        # Balance
  +        writer.writerow(['BALANCE'])
  +        balance = data.get('balance', {})
  +        for key, value in balance.items():
  +            writer.writerow([key, value])
  +        writer.writerow([])
  +        
  +        # Inventory
  +        writer.writerow(['INVENTORY'])
  +        writer.writerow(['Item ID', 'Quantity', 'Acquired At'])
  +        for item in data.get('inventory', []):
  +            writer.writerow([
  +                item.get('item_id'),
  +                item.get('quantity'),
  +                item.get('acquired_at')
  +            ])
  +    
  +    def _write_analytics_data_csv(self, writer: csv.writer, data: Dict[str, Any]):
  +        """Write analytics data to CSV"""
  +        writer.writerow(['ANALYTICS DATA EXPORT'])
  +        writer.writerow(['Generated At', datetime.now().isoformat()])
  +        time_range = data.get('time_range', {})
  +        writer.writerow(['Time Range', f"{time_range.get('start')} to {time_range.get('end')}"])
  +        writer.writerow([])
  +        
  +        # Aggregated metrics
  +        metrics = data.get('aggregated_metrics', {})
  +        for category, category_metrics in metrics.items():
  +            writer.writerow([category.upper()])
  +            for key, value in category_metrics.items():
  +                if isinstance(value, (int, float, str)):
  +                    writer.writerow([key, value])
  +                elif isinstance(value, dict):
  +                    writer.writerow([key])
  +                    for sub_key, sub_value in value.items():
  +                        writer.writerow(['', sub_key, sub_value])
  +                elif isinstance(value, list):
  +                    writer.writerow([key])
  +                    for item in value:
  +                        if isinstance(item, dict):
  +                            writer.writerow(['', json.dumps(item)])
  ```
  ----------------------------------------------------------

üìÑ ARCHIVO: modules/analytics/insights.py
============================================================
  üìç L√≠nea 163 | @gemini-code-assist[bot]:
  üí¨ ![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)

The import `from database.models import AnalyticsEvent` is done locally inside the method. This pattern is repeated in several methods in this file. It's generally better to place all imports at the top of the file for clarity, to avoid repeated imports, and to make dependencies explicit. If this is done to avoid circular dependencies, it might indicate a need to refactor the project structure.
  
  C√≥digo:
  ```
  @@ -0,0 +1,486 @@
  +"""
  +Insight Engine for DianaBot Analytics System
  +Automatically detects patterns and provides actionable insights
  +"""
  +
  +import logging
  +from datetime import datetime, timedelta
  +from typing import Dict, List, Optional, Any, Tuple
  +from dataclasses import dataclass
  +from sqlalchemy import func, and_, or_, text
  +from sqlalchemy.orm import Session
  +
  +logger = logging.getLogger(__name__)
  +
  +
  +@dataclass
  +class Insight:
  +    """Base insight data structure"""
  +    insight_type: str
  +    severity: str  # low, medium, high, critical
  +    title: str
  +    description: str
  +    recommendation: str
  +    data: Dict[str, Any]
  +    detected_at: datetime
  +
  +
  +@dataclass
  +class DropOffPoint:
  +    """Drop-off point in user journey"""
  +    point_id: str
  +    point_name: str
  +    drop_off_rate: float
  +    users_affected: int
  +    potential_revenue_loss: float
  +
  +
  +@dataclass
  +class HighValueUser:
  +    """High-value user identification"""
  +    user_id: int
  +    engagement_score: float
  +    conversion_probability: float
  +    potential_lifetime_value: float
  +    recommended_actions: List[str]
  +
  +
  +@dataclass
  +class ContentOptimization:
  +    """Content optimization suggestion"""
  +    content_id: str
  +    content_type: str
  +    current_performance: float
  +    suggested_improvement: str
  +    expected_impact: str
  +
  +
  +class InsightEngine:
  +    """Automatically detects patterns and provides actionable insights"""
  +    
  +    def __init__(self, db_session: Session):
  +        self.db = db_session
  +    
  +    def detect_drop_off_points(self, days_back: int = 30) -> List[DropOffPoint]:
  +        """Detect where users are dropping off in their journey"""
  +        logger.info(f"Detecting drop-off points for last {days_back} days")
  +        
  +        drop_off_points = []
  +        
  +        # Detect narrative drop-off points
  +        narrative_drop_offs = self._detect_narrative_drop_offs(days_back)
  +        drop_off_points.extend(narrative_drop_offs)
  +        
  +        # Detect experience drop-off points
  +        experience_drop_offs = self._detect_experience_drop_offs(days_back)
  +        drop_off_points.extend(experience_drop_offs)
  +        
  +        # Detect subscription funnel drop-offs
  +        subscription_drop_offs = self._detect_subscription_drop_offs(days_back)
  +        drop_off_points.extend(subscription_drop_offs)
  +        
  +        return drop_off_points
  +    
  +    def identify_high_value_users(self, days_back: int = 30) -> List[HighValueUser]:
  +        """Identify users with high conversion potential"""
  +        logger.info(f"Identifying high-value users for last {days_back} days")
  +        
  +        high_value_users = []
  +        
  +        # Get user engagement data
  +        user_engagement = self._get_user_engagement_data(days_back)
  +        
  +        # Calculate conversion probability for each user
  +        for user_id, engagement_data in user_engagement.items():
  +            conversion_prob = self._calculate_conversion_probability(user_id, engagement_data)
  +            potential_ltv = self._calculate_potential_ltv(engagement_data)
  +            
  +            # Only include users with high conversion probability
  +            if conversion_prob > 0.7:
  +                recommended_actions = self._get_recommended_actions(user_id, engagement_data)
  +                
  +                high_value_users.append(HighValueUser(
  +                    user_id=user_id,
  +                    engagement_score=engagement_data.get('engagement_score', 0),
  +                    conversion_probability=conversion_prob,
  +                    potential_lifetime_value=potential_ltv,
  +                    recommended_actions=recommended_actions
  +                ))
  +        
  +        # Sort by conversion probability (highest first)
  +        high_value_users.sort(key=lambda x: x.conversion_probability, reverse=True)
  +        
  +        return high_value_users[:20]  # Return top 20
  +    
  +    def suggest_content_optimizations(self, days_back: int = 30) -> List[ContentOptimization]:
  +        """Suggest content optimizations based on performance data"""
  +        logger.info(f"Suggesting content optimizations for last {days_back} days")
  +        
  +        optimizations = []
  +        
  +        # Analyze narrative content
  +        narrative_optimizations = self._analyze_narrative_content(days_back)
  +        optimizations.extend(narrative_optimizations)
  +        
  +        # Analyze experience content
  +        experience_optimizations = self._analyze_experience_content(days_back)
  +        optimizations.extend(experience_optimizations)
  +        
  +        # Analyze shop items
  +        shop_optimizations = self._analyze_shop_content(days_back)
  +        optimizations.extend(shop_optimizations)
  +        
  +        return optimizations
  +    
  +    def generate_insights(self, days_back: int = 30) -> List[Insight]:
  +        """Generate comprehensive insights across all areas"""
  +        logger.info(f"Generating comprehensive insights for last {days_back} days")
  +        
  +        insights = []
  +        
  +        # Generate engagement insights
  +        engagement_insights = self._generate_engagement_insights(days_back)
  +        insights.extend(engagement_insights)
  +        
  +        # Generate monetization insights
  +        monetization_insights = self._generate_monetization_insights(days_back)
  +        insights.extend(monetization_insights)
  +        
  +        # Generate content insights
  +        content_insights = self._generate_content_insights(days_back)
  +        insights.extend(content_insights)
  +        
  +        # Generate system insights
  +        system_insights = self._generate_system_insights(days_back)
  +        insights.extend(system_insights)
  +        
  +        return insights
  +    
  +    # Private helper methods
  +    
  +    def _detect_narrative_drop_offs(self, days_back: int) -> List[DropOffPoint]:
  +        """Detect drop-off points in narrative content"""
  +        from database.models import AnalyticsEvent
  ```
  ----------------------------------------------------------
  üìç L√≠nea 223 | @gemini-code-assist[bot]:
  üí¨ ![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)

The threshold `0.3` for identifying low completion rates is a magic number. It should be defined as a constant at the top of the file or in a configuration file, e.g., `LOW_COMPLETION_RATE_THRESHOLD = 0.3`. This improves readability and makes it easier to adjust the threshold in the future.
  
  C√≥digo:
  ```
  @@ -0,0 +1,486 @@
  +"""
  +Insight Engine for DianaBot Analytics System
  +Automatically detects patterns and provides actionable insights
  +"""
  +
  +import logging
  +from datetime import datetime, timedelta
  +from typing import Dict, List, Optional, Any, Tuple
  +from dataclasses import dataclass
  +from sqlalchemy import func, and_, or_, text
  +from sqlalchemy.orm import Session
  +
  +logger = logging.getLogger(__name__)
  +
  +
  +@dataclass
  +class Insight:
  +    """Base insight data structure"""
  +    insight_type: str
  +    severity: str  # low, medium, high, critical
  +    title: str
  +    description: str
  +    recommendation: str
  +    data: Dict[str, Any]
  +    detected_at: datetime
  +
  +
  +@dataclass
  +class DropOffPoint:
  +    """Drop-off point in user journey"""
  +    point_id: str
  +    point_name: str
  +    drop_off_rate: float
  +    users_affected: int
  +    potential_revenue_loss: float
  +
  +
  +@dataclass
  +class HighValueUser:
  +    """High-value user identification"""
  +    user_id: int
  +    engagement_score: float
  +    conversion_probability: float
  +    potential_lifetime_value: float
  +    recommended_actions: List[str]
  +
  +
  +@dataclass
  +class ContentOptimization:
  +    """Content optimization suggestion"""
  +    content_id: str
  +    content_type: str
  +    current_performance: float
  +    suggested_improvement: str
  +    expected_impact: str
  +
  +
  +class InsightEngine:
  +    """Automatically detects patterns and provides actionable insights"""
  +    
  +    def __init__(self, db_session: Session):
  +        self.db = db_session
  +    
  +    def detect_drop_off_points(self, days_back: int = 30) -> List[DropOffPoint]:
  +        """Detect where users are dropping off in their journey"""
  +        logger.info(f"Detecting drop-off points for last {days_back} days")
  +        
  +        drop_off_points = []
  +        
  +        # Detect narrative drop-off points
  +        narrative_drop_offs = self._detect_narrative_drop_offs(days_back)
  +        drop_off_points.extend(narrative_drop_offs)
  +        
  +        # Detect experience drop-off points
  +        experience_drop_offs = self._detect_experience_drop_offs(days_back)
  +        drop_off_points.extend(experience_drop_offs)
  +        
  +        # Detect subscription funnel drop-offs
  +        subscription_drop_offs = self._detect_subscription_drop_offs(days_back)
  +        drop_off_points.extend(subscription_drop_offs)
  +        
  +        return drop_off_points
  +    
  +    def identify_high_value_users(self, days_back: int = 30) -> List[HighValueUser]:
  +        """Identify users with high conversion potential"""
  +        logger.info(f"Identifying high-value users for last {days_back} days")
  +        
  +        high_value_users = []
  +        
  +        # Get user engagement data
  +        user_engagement = self._get_user_engagement_data(days_back)
  +        
  +        # Calculate conversion probability for each user
  +        for user_id, engagement_data in user_engagement.items():
  +            conversion_prob = self._calculate_conversion_probability(user_id, engagement_data)
  +            potential_ltv = self._calculate_potential_ltv(engagement_data)
  +            
  +            # Only include users with high conversion probability
  +            if conversion_prob > 0.7:
  +                recommended_actions = self._get_recommended_actions(user_id, engagement_data)
  +                
  +                high_value_users.append(HighValueUser(
  +                    user_id=user_id,
  +                    engagement_score=engagement_data.get('engagement_score', 0),
  +                    conversion_probability=conversion_prob,
  +                    potential_lifetime_value=potential_ltv,
  +                    recommended_actions=recommended_actions
  +                ))
  +        
  +        # Sort by conversion probability (highest first)
  +        high_value_users.sort(key=lambda x: x.conversion_probability, reverse=True)
  +        
  +        return high_value_users[:20]  # Return top 20
  +    
  +    def suggest_content_optimizations(self, days_back: int = 30) -> List[ContentOptimization]:
  +        """Suggest content optimizations based on performance data"""
  +        logger.info(f"Suggesting content optimizations for last {days_back} days")
  +        
  +        optimizations = []
  +        
  +        # Analyze narrative content
  +        narrative_optimizations = self._analyze_narrative_content(days_back)
  +        optimizations.extend(narrative_optimizations)
  +        
  +        # Analyze experience content
  +        experience_optimizations = self._analyze_experience_content(days_back)
  +        optimizations.extend(experience_optimizations)
  +        
  +        # Analyze shop items
  +        shop_optimizations = self._analyze_shop_content(days_back)
  +        optimizations.extend(shop_optimizations)
  +        
  +        return optimizations
  +    
  +    def generate_insights(self, days_back: int = 30) -> List[Insight]:
  +        """Generate comprehensive insights across all areas"""
  +        logger.info(f"Generating comprehensive insights for last {days_back} days")
  +        
  +        insights = []
  +        
  +        # Generate engagement insights
  +        engagement_insights = self._generate_engagement_insights(days_back)
  +        insights.extend(engagement_insights)
  +        
  +        # Generate monetization insights
  +        monetization_insights = self._generate_monetization_insights(days_back)
  +        insights.extend(monetization_insights)
  +        
  +        # Generate content insights
  +        content_insights = self._generate_content_insights(days_back)
  +        insights.extend(content_insights)
  +        
  +        # Generate system insights
  +        system_insights = self._generate_system_insights(days_back)
  +        insights.extend(system_insights)
  +        
  +        return insights
  +    
  +    # Private helper methods
  +    
  +    def _detect_narrative_drop_offs(self, days_back: int) -> List[DropOffPoint]:
  +        """Detect drop-off points in narrative content"""
  +        from database.models import AnalyticsEvent
  +        
  +        drop_off_points = []
  +        
  +        # Get narrative progression data
  +        narrative_progression = self.db.query(
  +            AnalyticsEvent.metadata['fragment_id'].astext.label('fragment_id'),
  +            func.count(AnalyticsEvent.id).label('views'),
  +            func.count(func.distinct(AnalyticsEvent.user_id)).label('unique_users')
  +        ).filter(
  +            and_(
  +                AnalyticsEvent.timestamp >= datetime.now() - timedelta(days=days_back),
  +                AnalyticsEvent.event_type == 'content_viewed',
  +                AnalyticsEvent.metadata['content_type'].astext == 'narrative'
  +            )
  +        ).group_by('fragment_id').all()
  +        
  +        # Calculate drop-off rates between consecutive fragments
  +        # This is a simplified implementation
  +        # In production, you'd need to track actual user progression
  +        
  +        return drop_off_points
  +    
  +    def _detect_experience_drop_offs(self, days_back: int) -> List[DropOffPoint]:
  +        """Detect drop-off points in experiences"""
  +        from database.models import AnalyticsEvent
  +        
  +        drop_off_points = []
  +        
  +        # Get experience start and completion data
  +        experience_data = self.db.query(
  +            AnalyticsEvent.metadata['experience_id'].astext.label('experience_id'),
  +            AnalyticsEvent.event_type,
  +            func.count(AnalyticsEvent.id).label('count')
  +        ).filter(
  +            and_(
  +                AnalyticsEvent.timestamp >= datetime.now() - timedelta(days=days_back),
  +                AnalyticsEvent.event_type.in_(['experience_started', 'experience_completed'])
  +            )
  +        ).group_by('experience_id', 'event_type').all()
  +        
  +        # Calculate completion rates
  +        experience_stats = {}
  +        for exp_id, event_type, count in experience_data:
  +            if exp_id not in experience_stats:
  +                experience_stats[exp_id] = {'started': 0, 'completed': 0}
  +            
  +            if event_type == 'experience_started':
  +                experience_stats[exp_id]['started'] = count
  +            elif event_type == 'experience_completed':
  +                experience_stats[exp_id]['completed'] = count
  +        
  +        # Identify experiences with low completion rates
  +        for exp_id, stats in experience_stats.items():
  +            started = stats['started']
  +            completed = stats['completed']
  +            
  +            if started > 0:
  +                completion_rate = completed / started
  +                
  +                if completion_rate < 0.3:  # Less than 30% completion
  ```
  ----------------------------------------------------------
  üìç L√≠nea 339 | @gemini-code-assist[bot]:
  üí¨ ![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)

The values `100`, `0.9`, `10`, and `0.2` are magic numbers used in the conversion probability calculation. They should be extracted into named constants to improve readability and maintainability. For example: `ENGAGEMENT_SCORE_NORMALIZATION = 100`, `MAX_BASE_PROBABILITY = 0.9`, etc.
  
  C√≥digo:
  ```
  @@ -0,0 +1,486 @@
  +"""
  +Insight Engine for DianaBot Analytics System
  +Automatically detects patterns and provides actionable insights
  +"""
  +
  +import logging
  +from datetime import datetime, timedelta
  +from typing import Dict, List, Optional, Any, Tuple
  +from dataclasses import dataclass
  +from sqlalchemy import func, and_, or_, text
  +from sqlalchemy.orm import Session
  +
  +logger = logging.getLogger(__name__)
  +
  +
  +@dataclass
  +class Insight:
  +    """Base insight data structure"""
  +    insight_type: str
  +    severity: str  # low, medium, high, critical
  +    title: str
  +    description: str
  +    recommendation: str
  +    data: Dict[str, Any]
  +    detected_at: datetime
  +
  +
  +@dataclass
  +class DropOffPoint:
  +    """Drop-off point in user journey"""
  +    point_id: str
  +    point_name: str
  +    drop_off_rate: float
  +    users_affected: int
  +    potential_revenue_loss: float
  +
  +
  +@dataclass
  +class HighValueUser:
  +    """High-value user identification"""
  +    user_id: int
  +    engagement_score: float
  +    conversion_probability: float
  +    potential_lifetime_value: float
  +    recommended_actions: List[str]
  +
  +
  +@dataclass
  +class ContentOptimization:
  +    """Content optimization suggestion"""
  +    content_id: str
  +    content_type: str
  +    current_performance: float
  +    suggested_improvement: str
  +    expected_impact: str
  +
  +
  +class InsightEngine:
  +    """Automatically detects patterns and provides actionable insights"""
  +    
  +    def __init__(self, db_session: Session):
  +        self.db = db_session
  +    
  +    def detect_drop_off_points(self, days_back: int = 30) -> List[DropOffPoint]:
  +        """Detect where users are dropping off in their journey"""
  +        logger.info(f"Detecting drop-off points for last {days_back} days")
  +        
  +        drop_off_points = []
  +        
  +        # Detect narrative drop-off points
  +        narrative_drop_offs = self._detect_narrative_drop_offs(days_back)
  +        drop_off_points.extend(narrative_drop_offs)
  +        
  +        # Detect experience drop-off points
  +        experience_drop_offs = self._detect_experience_drop_offs(days_back)
  +        drop_off_points.extend(experience_drop_offs)
  +        
  +        # Detect subscription funnel drop-offs
  +        subscription_drop_offs = self._detect_subscription_drop_offs(days_back)
  +        drop_off_points.extend(subscription_drop_offs)
  +        
  +        return drop_off_points
  +    
  +    def identify_high_value_users(self, days_back: int = 30) -> List[HighValueUser]:
  +        """Identify users with high conversion potential"""
  +        logger.info(f"Identifying high-value users for last {days_back} days")
  +        
  +        high_value_users = []
  +        
  +        # Get user engagement data
  +        user_engagement = self._get_user_engagement_data(days_back)
  +        
  +        # Calculate conversion probability for each user
  +        for user_id, engagement_data in user_engagement.items():
  +            conversion_prob = self._calculate_conversion_probability(user_id, engagement_data)
  +            potential_ltv = self._calculate_potential_ltv(engagement_data)
  +            
  +            # Only include users with high conversion probability
  +            if conversion_prob > 0.7:
  +                recommended_actions = self._get_recommended_actions(user_id, engagement_data)
  +                
  +                high_value_users.append(HighValueUser(
  +                    user_id=user_id,
  +                    engagement_score=engagement_data.get('engagement_score', 0),
  +                    conversion_probability=conversion_prob,
  +                    potential_lifetime_value=potential_ltv,
  +                    recommended_actions=recommended_actions
  +                ))
  +        
  +        # Sort by conversion probability (highest first)
  +        high_value_users.sort(key=lambda x: x.conversion_probability, reverse=True)
  +        
  +        return high_value_users[:20]  # Return top 20
  +    
  +    def suggest_content_optimizations(self, days_back: int = 30) -> List[ContentOptimization]:
  +        """Suggest content optimizations based on performance data"""
  +        logger.info(f"Suggesting content optimizations for last {days_back} days")
  +        
  +        optimizations = []
  +        
  +        # Analyze narrative content
  +        narrative_optimizations = self._analyze_narrative_content(days_back)
  +        optimizations.extend(narrative_optimizations)
  +        
  +        # Analyze experience content
  +        experience_optimizations = self._analyze_experience_content(days_back)
  +        optimizations.extend(experience_optimizations)
  +        
  +        # Analyze shop items
  +        shop_optimizations = self._analyze_shop_content(days_back)
  +        optimizations.extend(shop_optimizations)
  +        
  +        return optimizations
  +    
  +    def generate_insights(self, days_back: int = 30) -> List[Insight]:
  +        """Generate comprehensive insights across all areas"""
  +        logger.info(f"Generating comprehensive insights for last {days_back} days")
  +        
  +        insights = []
  +        
  +        # Generate engagement insights
  +        engagement_insights = self._generate_engagement_insights(days_back)
  +        insights.extend(engagement_insights)
  +        
  +        # Generate monetization insights
  +        monetization_insights = self._generate_monetization_insights(days_back)
  +        insights.extend(monetization_insights)
  +        
  +        # Generate content insights
  +        content_insights = self._generate_content_insights(days_back)
  +        insights.extend(content_insights)
  +        
  +        # Generate system insights
  +        system_insights = self._generate_system_insights(days_back)
  +        insights.extend(system_insights)
  +        
  +        return insights
  +    
  +    # Private helper methods
  +    
  +    def _detect_narrative_drop_offs(self, days_back: int) -> List[DropOffPoint]:
  +        """Detect drop-off points in narrative content"""
  +        from database.models import AnalyticsEvent
  +        
  +        drop_off_points = []
  +        
  +        # Get narrative progression data
  +        narrative_progression = self.db.query(
  +            AnalyticsEvent.metadata['fragment_id'].astext.label('fragment_id'),
  +            func.count(AnalyticsEvent.id).label('views'),
  +            func.count(func.distinct(AnalyticsEvent.user_id)).label('unique_users')
  +        ).filter(
  +            and_(
  +                AnalyticsEvent.timestamp >= datetime.now() - timedelta(days=days_back),
  +                AnalyticsEvent.event_type == 'content_viewed',
  +                AnalyticsEvent.metadata['content_type'].astext == 'narrative'
  +            )
  +        ).group_by('fragment_id').all()
  +        
  +        # Calculate drop-off rates between consecutive fragments
  +        # This is a simplified implementation
  +        # In production, you'd need to track actual user progression
  +        
  +        return drop_off_points
  +    
  +    def _detect_experience_drop_offs(self, days_back: int) -> List[DropOffPoint]:
  +        """Detect drop-off points in experiences"""
  +        from database.models import AnalyticsEvent
  +        
  +        drop_off_points = []
  +        
  +        # Get experience start and completion data
  +        experience_data = self.db.query(
  +            AnalyticsEvent.metadata['experience_id'].astext.label('experience_id'),
  +            AnalyticsEvent.event_type,
  +            func.count(AnalyticsEvent.id).label('count')
  +        ).filter(
  +            and_(
  +                AnalyticsEvent.timestamp >= datetime.now() - timedelta(days=days_back),
  +                AnalyticsEvent.event_type.in_(['experience_started', 'experience_completed'])
  +            )
  +        ).group_by('experience_id', 'event_type').all()
  +        
  +        # Calculate completion rates
  +        experience_stats = {}
  +        for exp_id, event_type, count in experience_data:
  +            if exp_id not in experience_stats:
  +                experience_stats[exp_id] = {'started': 0, 'completed': 0}
  +            
  +            if event_type == 'experience_started':
  +                experience_stats[exp_id]['started'] = count
  +            elif event_type == 'experience_completed':
  +                experience_stats[exp_id]['completed'] = count
  +        
  +        # Identify experiences with low completion rates
  +        for exp_id, stats in experience_stats.items():
  +            started = stats['started']
  +            completed = stats['completed']
  +            
  +            if started > 0:
  +                completion_rate = completed / started
  +                
  +                if completion_rate < 0.3:  # Less than 30% completion
  +                    drop_off_points.append(DropOffPoint(
  +                        point_id=exp_id,
  +                        point_name=f"Experience {exp_id}",
  +                        drop_off_rate=1 - completion_rate,
  +                        users_affected=started - completed,
  +                        potential_revenue_loss=0.0  # Would need business logic
  +                    ))
  +        
  +        return drop_off_points
  +    
  +    def _detect_subscription_drop_offs(self, days_back: int) -> List[DropOffPoint]:
  +        """Detect drop-off points in subscription funnel"""
  +        from database.models import ConversionFunnel
  +        
  +        drop_off_points = []
  +        
  +        # Get funnel stage data
  +        funnel_stages = self.db.query(
  +            ConversionFunnel.stage_current,
  +            func.count(ConversionFunnel.id).label('user_count')
  +        ).filter(
  +            and_(
  +                ConversionFunnel.entered_at >= datetime.now() - timedelta(days=days_back),
  +                ConversionFunnel.is_active == True
  +            )
  +        ).group_by(ConversionFunnel.stage_current).all()
  +        
  +        # Calculate drop-off between stages
  +        # This is a simplified implementation
  +        # In production, you'd track actual progression between stages
  +        
  +        return drop_off_points
  +    
  +    def _get_user_engagement_data(self, days_back: int) -> Dict[int, Dict[str, Any]]:
  +        """Get engagement data for all users"""
  +        from database.models import AnalyticsEvent
  +        
  +        user_engagement = {}
  +        
  +        # Get user activity counts
  +        user_activity = self.db.query(
  +            AnalyticsEvent.user_id,
  +            AnalyticsEvent.event_type,
  +            func.count(AnalyticsEvent.id).label('count')
  +        ).filter(
  +            AnalyticsEvent.timestamp >= datetime.now() - timedelta(days=days_back)
  +        ).group_by(AnalyticsEvent.user_id, AnalyticsEvent.event_type).all()
  +        
  +        # Aggregate by user
  +        for user_id, event_type, count in user_activity:
  +            if user_id not in user_engagement:
  +                user_engagement[user_id] = {
  +                    'total_events': 0,
  +                    'event_types': {},
  +                    'engagement_score': 0
  +                }
  +            
  +            user_engagement[user_id]['total_events'] += count
  +            user_engagement[user_id]['event_types'][event_type] = count
  +        
  +        # Calculate engagement scores
  +        for user_id, data in user_engagement.items():
  +            # Simple scoring based on event diversity and frequency
  +            event_diversity = len(data['event_types'])
  +            total_events = data['total_events']
  +            
  +            # Weight different event types
  +            weighted_score = 0
  +            for event_type, count in data['event_types'].items():
  +                weight = self._get_event_weight(event_type)
  +                weighted_score += count * weight
  +            
  +            data['engagement_score'] = weighted_score
  +        
  +        return user_engagement
  +    
  +    def _get_event_weight(self, event_type: str) -> float:
  +        """Get weight for event type in engagement scoring"""
  +        weights = {
  +            'user_login': 1.0,
  +            'content_viewed': 2.0,
  +            'reaction_added': 3.0,
  +            'mission_completed': 5.0,
  +            'achievement_unlocked': 8.0,
  +            'besitos_earned': 2.0,
  +            'trivia_answered': 3.0,
  +            'auction_participation': 4.0,
  +            'experience_started': 6.0,
  +            'experience_completed': 10.0
  +        }
  +        
  +        return weights.get(event_type, 1.0)
  +    
  +    def _calculate_conversion_probability(self, user_id: int, engagement_data: Dict[str, Any]) -> float:
  +        """Calculate probability that user will convert to VIP"""
  +        from database.models import Subscription
  +        
  +        # Check if user is already VIP
  +        existing_subscription = self.db.query(Subscription).filter(
  +            and_(
  +                Subscription.user_id == user_id,
  +                Subscription.status == 'active'
  +            )
  +        ).first()
  +        
  +        if existing_subscription:
  +            return 1.0  # Already converted
  +        
  +        # Calculate probability based on engagement
  +        engagement_score = engagement_data.get('engagement_score', 0)
  +        event_diversity = len(engagement_data.get('event_types', {}))
  +        
  +        # Simple probability calculation
  +        # In production, you'd use machine learning
  +        base_probability = min(engagement_score / 100, 0.9)  # Cap at 90%
  +        diversity_boost = min(event_diversity / 10, 0.2)  # Max 20% boost
  ```
  ----------------------------------------------------------

üìÑ ARCHIVO: modules/analytics/reports.py
============================================================
  üìç L√≠nea 172 | @gemini-code-assist[bot]:
  üí¨ ![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)

Report definitions are hardcoded. This makes it difficult to add, remove, or modify reports without changing the code. These definitions should be stored in a configuration file or a database table to allow for dynamic management of reports.
  
  C√≥digo:
  ```
  @@ -0,0 +1,393 @@
  +"""
  +Report Generation System for DianaBot Analytics
  +Generates automated reports in various formats
  +"""
  +
  +import logging
  +import json
  +import csv
  +import io
  +from datetime import datetime, timedelta
  +from typing import Dict, List, Optional, Any, Tuple
  +from dataclasses import dataclass
  +from sqlalchemy.orm import Session
  +
  +logger = logging.getLogger(__name__)
  +
  +
  +@dataclass
  +class ReportDefinition:
  +    """Report definition data structure"""
  +    report_id: str
  +    report_name: str
  +    report_type: str  # "executive", "revenue", "content", "experiences"
  +    frequency: str  # "daily", "weekly", "monthly"
  +    metrics: List[str]
  +    recipients: List[str]
  +    export_formats: List[str]  # "pdf", "excel", "csv", "json"
  +    is_active: bool = True
  +
  +
  +@dataclass
  +class GeneratedReport:
  +    """Generated report data structure"""
  +    report_id: str
  +    report_name: str
  +    generated_at: datetime
  +    time_range_start: datetime
  +    time_range_end: datetime
  +    data: Dict[str, Any]
  +    export_formats: Dict[str, str]  # format -> file_path or content
  +    status: str  # "generated", "failed", "processing"
  +
  +
  +class ReportGenerator:
  +    """Generates automated reports in various formats"""
  +    
  +    def __init__(self, db_session: Session):
  +        self.db = db_session
  +        self.report_definitions = self._load_report_definitions()
  +    
  +    def generate_report(self, report_id: str, time_range_start: datetime = None, 
  +                       time_range_end: datetime = None) -> GeneratedReport:
  +        """Generate a specific report"""
  +        logger.info(f"Generating report: {report_id}")
  +        
  +        report_def = self.report_definitions.get(report_id)
  +        if not report_def:
  +            raise ValueError(f"Report definition not found: {report_id}")
  +        
  +        if not report_def.is_active:
  +            raise ValueError(f"Report is not active: {report_id}")
  +        
  +        # Set default time range if not provided
  +        if time_range_start is None or time_range_end is None:
  +            time_range_start, time_range_end = self._get_default_time_range(report_def.frequency)
  +        
  +        # Generate report data
  +        report_data = self._generate_report_data(report_def, time_range_start, time_range_end)
  +        
  +        # Generate export formats
  +        export_formats = {}
  +        for format_type in report_def.export_formats:
  +            try:
  +                export_content = self._export_to_format(report_data, format_type, report_def)
  +                export_formats[format_type] = export_content
  +            except Exception as e:
  +                logger.error(f"Failed to export {report_id} to {format_type}: {e}")
  +        
  +        return GeneratedReport(
  +            report_id=report_id,
  +            report_name=report_def.report_name,
  +            generated_at=datetime.now(),
  +            time_range_start=time_range_start,
  +            time_range_end=time_range_end,
  +            data=report_data,
  +            export_formats=export_formats,
  +            status="generated"
  +        )
  +    
  +    def generate_all_scheduled_reports(self) -> List[GeneratedReport]:
  +        """Generate all scheduled reports based on frequency"""
  +        logger.info("Generating all scheduled reports")
  +        
  +        generated_reports = []
  +        
  +        for report_id, report_def in self.report_definitions.items():
  +            if not report_def.is_active:
  +                continue
  +            
  +            # Check if report should be generated based on frequency
  +            if self._should_generate_report(report_def):
  +                try:
  +                    report = self.generate_report(report_id)
  +                    generated_reports.append(report)
  +                    
  +                    # Send to recipients
  +                    self._send_report_to_recipients(report, report_def.recipients)
  +                    
  +                except Exception as e:
  +                    logger.error(f"Failed to generate report {report_id}: {e}")
  +        
  +        return generated_reports
  +    
  +    def get_report_definitions(self) -> Dict[str, ReportDefinition]:
  +        """Get all report definitions"""
  +        return self.report_definitions
  +    
  +    def update_report_definition(self, report_id: str, definition: ReportDefinition) -> bool:
  +        """Update report definition"""
  +        self.report_definitions[report_id] = definition
  +        return True
  +    
  +    def create_report_definition(self, definition: ReportDefinition) -> bool:
  +        """Create new report definition"""
  +        if definition.report_id in self.report_definitions:
  +            raise ValueError(f"Report ID already exists: {definition.report_id}")
  +        
  +        self.report_definitions[definition.report_id] = definition
  +        return True
  +    
  +    # Private helper methods
  +    
  +    def _load_report_definitions(self) -> Dict[str, ReportDefinition]:
  +        """Load report definitions"""
  +        return {
  +            'executive_weekly': ReportDefinition(
  +                report_id='executive_weekly',
  +                report_name='Weekly Executive Report',
  +                report_type='executive',
  +                frequency='weekly',
  +                metrics=['mau', 'dau', 'revenue', 'conversion_rate', 'ltv'],
  +                recipients=['admin@dianabot.com'],
  +                export_formats=['pdf', 'excel']
  +            ),
  +            'revenue_monthly': ReportDefinition(
  +                report_id='revenue_monthly',
  +                report_name='Monthly Revenue Report',
  +                report_type='revenue',
  +                frequency='monthly',
  +                metrics=['total_revenue', 'arpu', 'arppu', 'revenue_by_product'],
  +                recipients=['admin@dianabot.com', 'finance@dianabot.com'],
  +                export_formats=['excel', 'csv']
  +            ),
  +            'content_performance': ReportDefinition(
  +                report_id='content_performance',
  +                report_name='Content Performance Report',
  +                report_type='content',
  +                frequency='weekly',
  +                metrics=['narrative_metrics', 'experience_metrics', 'popular_content'],
  +                recipients=['admin@dianabot.com', 'content@dianabot.com'],
  +                export_formats=['excel', 'json']
  +            ),
  +            'experiences_weekly': ReportDefinition(
  +                report_id='experiences_weekly',
  +                report_name='Weekly Experiences Report',
  +                report_type='experiences',
  +                frequency='weekly',
  +                metrics=['experience_start_rate', 'completion_rate', 'popular_experiences'],
  +                recipients=['admin@dianabot.com'],
  +                export_formats=['csv', 'json']
  +            )
  +        }
  ```
  ----------------------------------------------------------
  üìç L√≠nea 202 | @gemini-code-assist[bot]:
  üí¨ ![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)

The import `from .aggregator import MetricsAggregator, TimeRange` is done locally inside the method. This pattern is repeated in this file. It's generally better to place all imports at the top of the file for clarity, to avoid repeated imports, and to make dependencies explicit. If this is done to avoid circular dependencies, it might indicate a need to refactor the project structure.
  
  C√≥digo:
  ```
  @@ -0,0 +1,393 @@
  +"""
  +Report Generation System for DianaBot Analytics
  +Generates automated reports in various formats
  +"""
  +
  +import logging
  +import json
  +import csv
  +import io
  +from datetime import datetime, timedelta
  +from typing import Dict, List, Optional, Any, Tuple
  +from dataclasses import dataclass
  +from sqlalchemy.orm import Session
  +
  +logger = logging.getLogger(__name__)
  +
  +
  +@dataclass
  +class ReportDefinition:
  +    """Report definition data structure"""
  +    report_id: str
  +    report_name: str
  +    report_type: str  # "executive", "revenue", "content", "experiences"
  +    frequency: str  # "daily", "weekly", "monthly"
  +    metrics: List[str]
  +    recipients: List[str]
  +    export_formats: List[str]  # "pdf", "excel", "csv", "json"
  +    is_active: bool = True
  +
  +
  +@dataclass
  +class GeneratedReport:
  +    """Generated report data structure"""
  +    report_id: str
  +    report_name: str
  +    generated_at: datetime
  +    time_range_start: datetime
  +    time_range_end: datetime
  +    data: Dict[str, Any]
  +    export_formats: Dict[str, str]  # format -> file_path or content
  +    status: str  # "generated", "failed", "processing"
  +
  +
  +class ReportGenerator:
  +    """Generates automated reports in various formats"""
  +    
  +    def __init__(self, db_session: Session):
  +        self.db = db_session
  +        self.report_definitions = self._load_report_definitions()
  +    
  +    def generate_report(self, report_id: str, time_range_start: datetime = None, 
  +                       time_range_end: datetime = None) -> GeneratedReport:
  +        """Generate a specific report"""
  +        logger.info(f"Generating report: {report_id}")
  +        
  +        report_def = self.report_definitions.get(report_id)
  +        if not report_def:
  +            raise ValueError(f"Report definition not found: {report_id}")
  +        
  +        if not report_def.is_active:
  +            raise ValueError(f"Report is not active: {report_id}")
  +        
  +        # Set default time range if not provided
  +        if time_range_start is None or time_range_end is None:
  +            time_range_start, time_range_end = self._get_default_time_range(report_def.frequency)
  +        
  +        # Generate report data
  +        report_data = self._generate_report_data(report_def, time_range_start, time_range_end)
  +        
  +        # Generate export formats
  +        export_formats = {}
  +        for format_type in report_def.export_formats:
  +            try:
  +                export_content = self._export_to_format(report_data, format_type, report_def)
  +                export_formats[format_type] = export_content
  +            except Exception as e:
  +                logger.error(f"Failed to export {report_id} to {format_type}: {e}")
  +        
  +        return GeneratedReport(
  +            report_id=report_id,
  +            report_name=report_def.report_name,
  +            generated_at=datetime.now(),
  +            time_range_start=time_range_start,
  +            time_range_end=time_range_end,
  +            data=report_data,
  +            export_formats=export_formats,
  +            status="generated"
  +        )
  +    
  +    def generate_all_scheduled_reports(self) -> List[GeneratedReport]:
  +        """Generate all scheduled reports based on frequency"""
  +        logger.info("Generating all scheduled reports")
  +        
  +        generated_reports = []
  +        
  +        for report_id, report_def in self.report_definitions.items():
  +            if not report_def.is_active:
  +                continue
  +            
  +            # Check if report should be generated based on frequency
  +            if self._should_generate_report(report_def):
  +                try:
  +                    report = self.generate_report(report_id)
  +                    generated_reports.append(report)
  +                    
  +                    # Send to recipients
  +                    self._send_report_to_recipients(report, report_def.recipients)
  +                    
  +                except Exception as e:
  +                    logger.error(f"Failed to generate report {report_id}: {e}")
  +        
  +        return generated_reports
  +    
  +    def get_report_definitions(self) -> Dict[str, ReportDefinition]:
  +        """Get all report definitions"""
  +        return self.report_definitions
  +    
  +    def update_report_definition(self, report_id: str, definition: ReportDefinition) -> bool:
  +        """Update report definition"""
  +        self.report_definitions[report_id] = definition
  +        return True
  +    
  +    def create_report_definition(self, definition: ReportDefinition) -> bool:
  +        """Create new report definition"""
  +        if definition.report_id in self.report_definitions:
  +            raise ValueError(f"Report ID already exists: {definition.report_id}")
  +        
  +        self.report_definitions[definition.report_id] = definition
  +        return True
  +    
  +    # Private helper methods
  +    
  +    def _load_report_definitions(self) -> Dict[str, ReportDefinition]:
  +        """Load report definitions"""
  +        return {
  +            'executive_weekly': ReportDefinition(
  +                report_id='executive_weekly',
  +                report_name='Weekly Executive Report',
  +                report_type='executive',
  +                frequency='weekly',
  +                metrics=['mau', 'dau', 'revenue', 'conversion_rate', 'ltv'],
  +                recipients=['admin@dianabot.com'],
  +                export_formats=['pdf', 'excel']
  +            ),
  +            'revenue_monthly': ReportDefinition(
  +                report_id='revenue_monthly',
  +                report_name='Monthly Revenue Report',
  +                report_type='revenue',
  +                frequency='monthly',
  +                metrics=['total_revenue', 'arpu', 'arppu', 'revenue_by_product'],
  +                recipients=['admin@dianabot.com', 'finance@dianabot.com'],
  +                export_formats=['excel', 'csv']
  +            ),
  +            'content_performance': ReportDefinition(
  +                report_id='content_performance',
  +                report_name='Content Performance Report',
  +                report_type='content',
  +                frequency='weekly',
  +                metrics=['narrative_metrics', 'experience_metrics', 'popular_content'],
  +                recipients=['admin@dianabot.com', 'content@dianabot.com'],
  +                export_formats=['excel', 'json']
  +            ),
  +            'experiences_weekly': ReportDefinition(
  +                report_id='experiences_weekly',
  +                report_name='Weekly Experiences Report',
  +                report_type='experiences',
  +                frequency='weekly',
  +                metrics=['experience_start_rate', 'completion_rate', 'popular_experiences'],
  +                recipients=['admin@dianabot.com'],
  +                export_formats=['csv', 'json']
  +            )
  +        }
  +    
  +    def _get_default_time_range(self, frequency: str) -> Tuple[datetime, datetime]:
  +        """Get default time range based on frequency"""
  +        now = datetime.now()
  +        
  +        if frequency == 'daily':
  +            start = now - timedelta(days=1)
  +            end = now
  +        elif frequency == 'weekly':
  +            start = now - timedelta(days=7)
  +            end = now
  +        elif frequency == 'monthly':
  +            start = now - timedelta(days=30)
  +            end = now
  +        else:
  +            start = now - timedelta(days=7)
  +            end = now
  +        
  +        return start, end
  +    
  +    def _should_generate_report(self, report_def: ReportDefinition) -> bool:
  +        """Check if report should be generated based on frequency"""
  +        # In production, this would check last generation time
  +        # For now, always return True for demonstration
  +        return True
  +    
  +    def _generate_report_data(self, report_def: ReportDefinition, 
  +                            time_range_start: datetime, time_range_end: datetime) -> Dict[str, Any]:
  +        """Generate report data based on metrics"""
  +        from .aggregator import MetricsAggregator, TimeRange
  ```
  ----------------------------------------------------------
  üìç L√≠nea 348 | @gemini-code-assist[bot]:
  üí¨ ![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)

Embedding a JSON string within a CSV cell (`json.dumps(item)`) can make the resulting CSV file difficult to parse and analyze with standard tools. It would be better to flatten the nested dictionary into separate columns or represent the list in a more CSV-friendly way, for example by creating more rows.
  
  C√≥digo:
  ```
  @@ -0,0 +1,393 @@
  +"""
  +Report Generation System for DianaBot Analytics
  +Generates automated reports in various formats
  +"""
  +
  +import logging
  +import json
  +import csv
  +import io
  +from datetime import datetime, timedelta
  +from typing import Dict, List, Optional, Any, Tuple
  +from dataclasses import dataclass
  +from sqlalchemy.orm import Session
  +
  +logger = logging.getLogger(__name__)
  +
  +
  +@dataclass
  +class ReportDefinition:
  +    """Report definition data structure"""
  +    report_id: str
  +    report_name: str
  +    report_type: str  # "executive", "revenue", "content", "experiences"
  +    frequency: str  # "daily", "weekly", "monthly"
  +    metrics: List[str]
  +    recipients: List[str]
  +    export_formats: List[str]  # "pdf", "excel", "csv", "json"
  +    is_active: bool = True
  +
  +
  +@dataclass
  +class GeneratedReport:
  +    """Generated report data structure"""
  +    report_id: str
  +    report_name: str
  +    generated_at: datetime
  +    time_range_start: datetime
  +    time_range_end: datetime
  +    data: Dict[str, Any]
  +    export_formats: Dict[str, str]  # format -> file_path or content
  +    status: str  # "generated", "failed", "processing"
  +
  +
  +class ReportGenerator:
  +    """Generates automated reports in various formats"""
  +    
  +    def __init__(self, db_session: Session):
  +        self.db = db_session
  +        self.report_definitions = self._load_report_definitions()
  +    
  +    def generate_report(self, report_id: str, time_range_start: datetime = None, 
  +                       time_range_end: datetime = None) -> GeneratedReport:
  +        """Generate a specific report"""
  +        logger.info(f"Generating report: {report_id}")
  +        
  +        report_def = self.report_definitions.get(report_id)
  +        if not report_def:
  +            raise ValueError(f"Report definition not found: {report_id}")
  +        
  +        if not report_def.is_active:
  +            raise ValueError(f"Report is not active: {report_id}")
  +        
  +        # Set default time range if not provided
  +        if time_range_start is None or time_range_end is None:
  +            time_range_start, time_range_end = self._get_default_time_range(report_def.frequency)
  +        
  +        # Generate report data
  +        report_data = self._generate_report_data(report_def, time_range_start, time_range_end)
  +        
  +        # Generate export formats
  +        export_formats = {}
  +        for format_type in report_def.export_formats:
  +            try:
  +                export_content = self._export_to_format(report_data, format_type, report_def)
  +                export_formats[format_type] = export_content
  +            except Exception as e:
  +                logger.error(f"Failed to export {report_id} to {format_type}: {e}")
  +        
  +        return GeneratedReport(
  +            report_id=report_id,
  +            report_name=report_def.report_name,
  +            generated_at=datetime.now(),
  +            time_range_start=time_range_start,
  +            time_range_end=time_range_end,
  +            data=report_data,
  +            export_formats=export_formats,
  +            status="generated"
  +        )
  +    
  +    def generate_all_scheduled_reports(self) -> List[GeneratedReport]:
  +        """Generate all scheduled reports based on frequency"""
  +        logger.info("Generating all scheduled reports")
  +        
  +        generated_reports = []
  +        
  +        for report_id, report_def in self.report_definitions.items():
  +            if not report_def.is_active:
  +                continue
  +            
  +            # Check if report should be generated based on frequency
  +            if self._should_generate_report(report_def):
  +                try:
  +                    report = self.generate_report(report_id)
  +                    generated_reports.append(report)
  +                    
  +                    # Send to recipients
  +                    self._send_report_to_recipients(report, report_def.recipients)
  +                    
  +                except Exception as e:
  +                    logger.error(f"Failed to generate report {report_id}: {e}")
  +        
  +        return generated_reports
  +    
  +    def get_report_definitions(self) -> Dict[str, ReportDefinition]:
  +        """Get all report definitions"""
  +        return self.report_definitions
  +    
  +    def update_report_definition(self, report_id: str, definition: ReportDefinition) -> bool:
  +        """Update report definition"""
  +        self.report_definitions[report_id] = definition
  +        return True
  +    
  +    def create_report_definition(self, definition: ReportDefinition) -> bool:
  +        """Create new report definition"""
  +        if definition.report_id in self.report_definitions:
  +            raise ValueError(f"Report ID already exists: {definition.report_id}")
  +        
  +        self.report_definitions[definition.report_id] = definition
  +        return True
  +    
  +    # Private helper methods
  +    
  +    def _load_report_definitions(self) -> Dict[str, ReportDefinition]:
  +        """Load report definitions"""
  +        return {
  +            'executive_weekly': ReportDefinition(
  +                report_id='executive_weekly',
  +                report_name='Weekly Executive Report',
  +                report_type='executive',
  +                frequency='weekly',
  +                metrics=['mau', 'dau', 'revenue', 'conversion_rate', 'ltv'],
  +                recipients=['admin@dianabot.com'],
  +                export_formats=['pdf', 'excel']
  +            ),
  +            'revenue_monthly': ReportDefinition(
  +                report_id='revenue_monthly',
  +                report_name='Monthly Revenue Report',
  +                report_type='revenue',
  +                frequency='monthly',
  +                metrics=['total_revenue', 'arpu', 'arppu', 'revenue_by_product'],
  +                recipients=['admin@dianabot.com', 'finance@dianabot.com'],
  +                export_formats=['excel', 'csv']
  +            ),
  +            'content_performance': ReportDefinition(
  +                report_id='content_performance',
  +                report_name='Content Performance Report',
  +                report_type='content',
  +                frequency='weekly',
  +                metrics=['narrative_metrics', 'experience_metrics', 'popular_content'],
  +                recipients=['admin@dianabot.com', 'content@dianabot.com'],
  +                export_formats=['excel', 'json']
  +            ),
  +            'experiences_weekly': ReportDefinition(
  +                report_id='experiences_weekly',
  +                report_name='Weekly Experiences Report',
  +                report_type='experiences',
  +                frequency='weekly',
  +                metrics=['experience_start_rate', 'completion_rate', 'popular_experiences'],
  +                recipients=['admin@dianabot.com'],
  +                export_formats=['csv', 'json']
  +            )
  +        }
  +    
  +    def _get_default_time_range(self, frequency: str) -> Tuple[datetime, datetime]:
  +        """Get default time range based on frequency"""
  +        now = datetime.now()
  +        
  +        if frequency == 'daily':
  +            start = now - timedelta(days=1)
  +            end = now
  +        elif frequency == 'weekly':
  +            start = now - timedelta(days=7)
  +            end = now
  +        elif frequency == 'monthly':
  +            start = now - timedelta(days=30)
  +            end = now
  +        else:
  +            start = now - timedelta(days=7)
  +            end = now
  +        
  +        return start, end
  +    
  +    def _should_generate_report(self, report_def: ReportDefinition) -> bool:
  +        """Check if report should be generated based on frequency"""
  +        # In production, this would check last generation time
  +        # For now, always return True for demonstration
  +        return True
  +    
  +    def _generate_report_data(self, report_def: ReportDefinition, 
  +                            time_range_start: datetime, time_range_end: datetime) -> Dict[str, Any]:
  +        """Generate report data based on metrics"""
  +        from .aggregator import MetricsAggregator, TimeRange
  +        
  +        aggregator = MetricsAggregator(self.db)
  +        time_range = TimeRange(time_range_start, time_range_end)
  +        
  +        report_data = {
  +            'report_info': {
  +                'report_id': report_def.report_id,
  +                'report_name': report_def.report_name,
  +                'time_range': {
  +                    'start': time_range_start.isoformat(),
  +                    'end': time_range_end.isoformat()
  +                },
  +                'generated_at': datetime.now().isoformat()
  +            },
  +            'metrics': {}
  +        }
  +        
  +        # Generate metrics based on report type
  +        if report_def.report_type == 'executive':
  +            engagement_metrics = aggregator.get_engagement_metrics(time_range)
  +            monetization_metrics = aggregator.get_monetization_metrics(time_range)
  +            
  +            report_data['metrics']['engagement'] = {
  +                'mau': engagement_metrics.mau,
  +                'dau': engagement_metrics.dau,
  +                'retention_d1': engagement_metrics.retention_d1,
  +                'retention_d7': engagement_metrics.retention_d7,
  +                'retention_d30': engagement_metrics.retention_d30,
  +                'avg_session_duration': engagement_metrics.avg_session_duration
  +            }
  +            
  +            report_data['metrics']['monetization'] = {
  +                'total_revenue': monetization_metrics.total_revenue,
  +                'arpu': monetization_metrics.arpu,
  +                'arppu': monetization_metrics.arppu,
  +                'free_to_vip_conversion': monetization_metrics.free_to_vip_conversion,
  +                'ltv': monetization_metrics.ltv
  +            }
  +            
  +        elif report_def.report_type == 'revenue':
  +            monetization_metrics = aggregator.get_monetization_metrics(time_range)
  +            
  +            report_data['metrics']['revenue'] = {
  +                'total_revenue': monetization_metrics.total_revenue,
  +                'arpu': monetization_metrics.arpu,
  +                'arppu': monetization_metrics.arppu,
  +                'revenue_by_product': monetization_metrics.revenue_by_product
  +            }
  +            
  +        elif report_def.report_type == 'content':
  +            narrative_metrics = aggregator.get_narrative_metrics(time_range)
  +            experience_metrics = aggregator.get_experience_metrics(time_range)
  +            
  +            report_data['metrics']['narrative'] = {
  +                'most_visited_fragments': narrative_metrics.most_visited_fragments,
  +                'completion_rate': narrative_metrics.completion_rate,
  +                'popular_decisions': narrative_metrics.popular_decisions
  +            }
  +            
  +            report_data['metrics']['experiences'] = {
  +                'start_rate': experience_metrics.start_rate,
  +                'completion_rate': experience_metrics.completion_rate,
  +                'avg_completion_time': experience_metrics.avg_completion_time,
  +                'popular_experiences': experience_metrics.popular_experiences
  +            }
  +            
  +        elif report_def.report_type == 'experiences':
  +            experience_metrics = aggregator.get_experience_metrics(time_range)
  +            
  +            report_data['metrics']['experiences'] = {
  +                'start_rate': experience_metrics.start_rate,
  +                'completion_rate': experience_metrics.completion_rate,
  +                'avg_completion_time': experience_metrics.avg_completion_time,
  +                'popular_experiences': experience_metrics.popular_experiences
  +            }
  +        
  +        # Add insights if available
  +        from .insights import InsightEngine
  +        insight_engine = InsightEngine(self.db)
  +        insights = insight_engine.generate_insights(days_back=7)
  +        
  +        if insights:
  +            report_data['insights'] = [
  +                {
  +                    'type': insight.insight_type,
  +                    'severity': insight.severity,
  +                    'title': insight.title,
  +                    'description': insight.description,
  +                    'recommendation': insight.recommendation
  +                }
  +                for insight in insights
  +            ]
  +        
  +        return report_data
  +    
  +    def _export_to_format(self, report_data: Dict[str, Any], format_type: str, 
  +                         report_def: ReportDefinition) -> str:
  +        """Export report data to specified format"""
  +        if format_type == 'json':
  +            return json.dumps(report_data, indent=2, ensure_ascii=False)
  +        
  +        elif format_type == 'csv':
  +            return self._export_to_csv(report_data, report_def)
  +        
  +        elif format_type == 'excel':
  +            # In production, this would use a library like openpyxl
  +            # For now, return CSV as placeholder
  +            return self._export_to_csv(report_data, report_def)
  +        
  +        elif format_type == 'pdf':
  +            # In production, this would use a library like reportlab
  +            # For now, return JSON as placeholder
  +            return json.dumps(report_data, indent=2, ensure_ascii=False)
  +        
  +        else:
  +            raise ValueError(f"Unsupported export format: {format_type}")
  +    
  +    def _export_to_csv(self, report_data: Dict[str, Any], report_def: ReportDefinition) -> str:
  +        """Export report data to CSV format"""
  +        output = io.StringIO()
  +        writer = csv.writer(output)
  +        
  +        # Write header
  +        writer.writerow(['Report', report_def.report_name])
  +        writer.writerow(['Generated At', report_data['report_info']['generated_at']])
  +        writer.writerow(['Time Range', 
  +                        f"{report_data['report_info']['time_range']['start']} to {report_data['report_info']['time_range']['end']}"])
  +        writer.writerow([])
  +        
  +        # Write metrics
  +        for category, metrics in report_data['metrics'].items():
  +            writer.writerow([category.upper()])
  +            
  +            if isinstance(metrics, dict):
  +                for key, value in metrics.items():
  +                    if isinstance(value, (int, float, str)):
  +                        writer.writerow([key, value])
  +                    elif isinstance(value, dict):
  +                        writer.writerow([key])
  +                        for sub_key, sub_value in value.items():
  +                            writer.writerow(['', sub_key, sub_value])
  +                    elif isinstance(value, list):
  +                        writer.writerow([key])
  +                        for item in value:
  +                            if isinstance(item, dict):
  +                                writer.writerow(['', json.dumps(item)])
  ```
  ----------------------------------------------------------


============================================================
RESUMEN:
------------------------------------------------------------
  üìù Comentarios en conversaci√≥n: 1
  ‚≠ê Reviews generales: 1
  üìÑ Comentarios en c√≥digo: 23
============================================================
